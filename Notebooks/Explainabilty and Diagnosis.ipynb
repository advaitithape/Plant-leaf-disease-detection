{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9629432,"sourceType":"datasetVersion","datasetId":5846888}],"dockerImageVersionId":31239,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **1. Objective of This Notebook**\n\nThis notebook answers four critical questions:\n\n* How well does the model perform per class?\n\n* What visual regions does the model attend to?\n\n* Are learned representations well separated?\n\n* Can the model’s confidence be trusted?\n\nTogether, these analyses help assess model reliability, not just accuracy.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## **3. Quantitative Evaluation (Metrics & Confusion Matrix)**","metadata":{}},{"cell_type":"markdown","source":"3.1 Collect Predictions Safely","metadata":{}},{"cell_type":"code","source":"def collect_logits_labels(model, loader, device):\n    model.eval()\n    logits_list, labels_list = [], []\n    with torch.no_grad(), torch.amp.autocast(device_type='cuda', enabled=(device.type=='cuda')):\n        for x, y in loader:\n            x = x.to(device, non_blocking=True)\n            out = model(x).float().cpu()\n            logits_list.append(out)\n            labels_list.append(y.cpu())\n    logits = torch.cat(logits_list)\n    labels = torch.cat(labels_list)\n    probs  = torch.softmax(logits, dim=1).numpy()\n    preds  = probs.argmax(1)\n    return probs, preds, labels.numpy()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = LabelSmoothingCE(0.1)\nva_loss, va_acc, va_f1, va_top3, va_cm = validate(model, val_loader, criterion, DEVICE)\nprobs, preds, labels = collect_logits_labels(model, val_loader, DEVICE)\n\nprint(f\"Val loss: {va_loss:.4f} | Acc: {va_acc:.4f} | F1: {va_f1:.4f} | Top-3: {va_top3:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"3.2 Per-Class Performance Report","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nimport seaborn as sns, matplotlib.pyplot as plt\n\ntarget_names = [idx2label[i] for i in range(len(idx2label))]\nreport = classification_report(labels, preds, target_names=target_names, output_dict=True, zero_division=0)\npd.DataFrame(report).transpose().to_csv(FIG_DIR / \"per_class_report.csv\")\nprint(\"Saved per_class_report.csv\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12,10))\nsns.heatmap(va_cm, cmap=\"Blues\", cbar=True)\nplt.title(\"Confusion Matrix (Validation)\")\nplt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\nplt.tight_layout()\nplt.savefig(FIG_DIR / \"confusion_matrix_val.png\", dpi=200)\nplt.close()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **4. Spatial Explainability — Grad-CAM**\n4.1 Motivation\n\nGrad-CAM highlights where the model looks when making predictions, helping detect:\n\n* background bias\n* shortcut learning\n* disease-region focus","metadata":{}},{"cell_type":"markdown","source":"4.2 Grad-CAM Visualization","metadata":{}},{"cell_type":"code","source":"!pip -q install torchcam==0.4.0\nfrom torchcam.methods import GradCAM\nimport cv2, matplotlib.pyplot as plt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def overlay_cam(img_rgb, cam, alpha=0.4):\n    cam = cv2.resize(cam, (img_rgb.shape[1], img_rgb.shape[0]))\n    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-6)\n    heat = cv2.applyColorMap((255*cam).astype(np.uint8), cv2.COLORMAP_JET)\n    heat = heat[..., ::-1]\n    return (alpha*heat + (1-alpha)*img_rgb).astype(np.uint8)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# find late conv layer for EfficientNet-B3\ntarget_layer = None\nfor n, m in model.named_modules():\n    if \"blocks.6\" in n and hasattr(m, \"conv_pw\"):\n        target_layer = n\n\ncam_extractor = GradCAM(model, target_layer)\nsamples = val_df.sample(8, random_state=42)\ntf = get_val_transform()\n\nfig, axes = plt.subplots(4, 2, figsize=(10,18))\naxes = axes.flatten()\n\nfor ax, (_, row) in zip(axes, samples.iterrows()):\n    img = cv2.imread(row.filepath)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    x = tf(image=img)[\"image\"].unsqueeze(0).to(DEVICE)\n    out = model(x)\n    cls = out.argmax(1).item()\n    cam = cam_extractor(cls, out)[0].detach().cpu().numpy()\n    ax.imshow(overlay_cam(img, cam))\n    ax.set_title(f\"Pred: {idx2label[cls]}\")\n    ax.axis(\"off\")\n\nplt.tight_layout()\nplt.savefig(FIG_DIR / \"gradcam_samples.png\", dpi=200)\nplt.close()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **5. Representation Analysis — UMAP**\n5.1 Motivation\n\nUMAP reveals:\n\n* class separability\n* overlapping disease clusters\n* representation quality beyond accuracy","metadata":{}},{"cell_type":"markdown","source":"5.2 Safe Feature Extraction","metadata":{}},{"cell_type":"code","source":"import umap, gc\n\ndef extract_features(model, loader, device):\n    model.eval()\n    feats, labs = [], []\n    for x, y in loader:\n        x = x.to(device, non_blocking=True)\n        with torch.amp.autocast(device_type='cuda', enabled=(device.type=='cuda')):\n            f = model.global_pool(model.forward_features(x)).detach()\n        feats.append(f.cpu().numpy())\n        labs.append(y.numpy())\n    return np.concatenate(feats), np.concatenate(labs)\n\nfeats, labs = extract_features(model, val_loader, DEVICE)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emb = umap.UMAP(n_neighbors=15, min_dist=0.1, metric=\"cosine\", random_state=42).fit_transform(feats)\n\nplt.figure(figsize=(10,8))\nfor i in range(len(idx2label)):\n    m = labs == i\n    if m.sum():\n        plt.scatter(emb[m,0], emb[m,1], s=6, alpha=0.6, label=idx2label[i])\nplt.legend(markerscale=4, bbox_to_anchor=(1.05,1), fontsize=7)\nplt.title(\"UMAP Projection (Validation)\")\nplt.tight_layout()\nplt.savefig(FIG_DIR / \"umap_val.png\", dpi=200)\nplt.close()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **6. Confidence Calibration — Temperature Scaling**\n6.1 Why Calibration Matters\n\nHigh accuracy ≠ reliable confidence.\n\nWe measure this using Expected Calibration Error (ECE).","metadata":{}},{"cell_type":"markdown","source":"6.2 Compute ECE & Apply Temperature Scaling","metadata":{}},{"cell_type":"code","source":"def compute_ece(probs, labels, n_bins=15):\n    conf = probs.max(1)\n    preds = probs.argmax(1)\n    acc = (preds == labels).astype(float)\n    bins = np.linspace(0,1,n_bins+1)\n    ece = 0.0\n    for i in range(n_bins):\n        m = (conf > bins[i]) & (conf <= bins[i+1])\n        if m.any():\n            ece += m.mean() * abs(acc[m].mean() - conf[m].mean())\n    return ece\n\nece_before = compute_ece(probs, labels)\nprint(\"ECE before:\", round(ece_before, 4))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"T = torch.nn.Parameter(torch.ones(1), requires_grad=True)\nopt = torch.optim.LBFGS([T], lr=0.1, max_iter=50)\n\nlogits = torch.tensor(probs).log()\nlabels_t = torch.tensor(labels)\n\ndef closure():\n    opt.zero_grad()\n    loss = torch.nn.functional.cross_entropy(logits / T.clamp(0.05,10), labels_t)\n    loss.backward()\n    return loss\n\nopt.step(closure)\n\nT_final = float(T.item())\nprobs_scaled = torch.softmax(logits / T_final, dim=1).numpy()\nece_after = compute_ece(probs_scaled, labels)\n\nprint(\"ECE after:\", round(ece_after, 4))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(CKPT_DIR / \"temperature.txt\", \"w\") as f:\n    f.write(str(T_final))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **7. Failure Analysis — Misclassification Export**\n7.1 Motivation\n\nWe export misclassifications with contextual cues to identify:\n\n* lighting issues\n* aspect ratio bias\n* black borders","metadata":{}},{"cell_type":"markdown","source":"7.2 Export Misclassified Samples","metadata":{}},{"cell_type":"code","source":"import cv2\n\ndef context_tags(fp):\n    img = cv2.imread(fp)\n    if img is None:\n        return {\"black_bars\": False, \"aspect\": \"unknown\", \"bright\": 0.0, \"sat\": 0.0}\n    h,w = img.shape[:2]\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n    return {\n        \"black_bars\": (gray[:,0]<5).mean()>0.9,\n        \"aspect\": \"tall\" if h/w>1.2 else \"wide\" if w/h>1.2 else \"square\",\n        \"bright\": round(hsv[...,2].mean()/255,3),\n        \"sat\": round(hsv[...,1].mean()/255,3)\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rows = []\npaths = val_df[\"filepath\"].tolist()\n\nfor fp, y, p, conf in zip(paths, labels, preds, probs.max(1)):\n    if y != p:\n        rows.append({\n            \"filepath\": fp,\n            \"true\": idx2label[int(y)],\n            \"pred\": idx2label[int(p)],\n            \"confidence\": float(conf),\n            **context_tags(fp)\n        })\n\npd.DataFrame(rows).to_csv(FIG_DIR / \"misclassifications_val.csv\", index=False)\nprint(\"Saved misclassifications_val.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **8. Key Takeaways**\n\n* The model attends to disease regions, not backgrounds (Grad-CAM)\n\n* Some disease classes overlap in feature space (UMAP)\n\n* Calibration significantly improves confidence reliability\n\n* Misclassifications correlate with image quality issues","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}