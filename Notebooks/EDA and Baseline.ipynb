{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Plant Leaf Disease Detection**\n## Phase 1: Dataset Audit, Cleaning, and Baseline Modeling","metadata":{}},{"cell_type":"markdown","source":"### 1. **Dataset Audit & Quality Profiling**\n   \nðŸ“Œ Objective\n\nBefore training any model, we validate dataset quality:\n* Remove corrupt images\n* Detect duplicates\n* Identify low-quality samples\n* Check for background bias (lab bias risk)\nThis ensures the model learns leaf disease patterns, not noise or background shortcuts.","metadata":{}},{"cell_type":"markdown","source":"Cell 1 â€” Dataset Audit Pipeline","metadata":{}},{"cell_type":"code","source":"# ===========================\n# Dataset Audit & Quality Analysis\n# ===========================\n\nimport os\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom PIL import Image\nimport imagehash\nfrom pathlib import Path\nfrom collections import Counter\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Paths\nDATA_DIR = Path(\"/kaggle/input/plant-disease-detection-dataset-master-version/MasterDataset\")\nREPORT_DIR = Path(\"outputs/audit_report\")\nREPORT_DIR.mkdir(parents=True, exist_ok=True)\n\n# ---------- Helper Functions ----------\ndef is_image_ok(img_path):\n    \"\"\"Check whether image is readable and valid RGB.\"\"\"\n    try:\n        Image.open(img_path).convert(\"RGB\")\n        return True\n    except:\n        return False\n\ndef image_stats(img_path):\n    \"\"\"Compute brightness, contrast, and sharpness.\"\"\"\n    img = cv2.imread(str(img_path))\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    brightness = gray.mean()\n    contrast = gray.std()\n    sharpness = cv2.Laplacian(gray, cv2.CV_64F).var()\n    return brightness, contrast, sharpness\n\ndef compute_phash(img_path):\n    \"\"\"Perceptual hash for duplicate detection.\"\"\"\n    try:\n        return str(imagehash.phash(Image.open(img_path).convert(\"RGB\")))\n    except:\n        return None\n\ndef background_features(img_path):\n    \"\"\"Simple background embedding (mean + std color).\"\"\"\n    img = cv2.imread(str(img_path))\n    img = cv2.resize(img, (64, 64))\n    return np.concatenate([img.mean((0,1)), img.std((0,1))])\n\n# ---------- Audit Loop ----------\nrecords = []\nremoved_files = 0\n\nfor split in [\"train\", \"val\", \"test\"]:\n    split_dir = DATA_DIR / split\n    if not split_dir.exists():\n        continue\n\n    for cls in sorted(os.listdir(split_dir)):\n        cls_dir = split_dir / cls\n        if not cls_dir.is_dir():\n            continue\n\n        for img_path in tqdm(cls_dir.glob(\"*\"), desc=f\"{split}/{cls}\"):\n            if not img_path.is_file():\n                continue\n\n            if not is_image_ok(img_path):\n                img_path.unlink()\n                removed_files += 1\n                continue\n\n            b, c, s = image_stats(img_path)\n            ph = compute_phash(img_path)\n            bg = background_features(img_path)\n\n            records.append([split, cls, img_path.name, b, c, s, ph, bg])\n\ndf = pd.DataFrame(records, columns=[\n    \"split\", \"class\", \"filename\",\n    \"brightness\", \"contrast\", \"sharpness\",\n    \"phash\", \"bg_feats\"\n])\n\nprint(f\"Removed {removed_files} corrupt images\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Duplicate & Quality Outlier Detection","metadata":{}},{"cell_type":"code","source":"# ---------- Duplicate Detection ----------\ndup_counts = Counter(df[\"phash\"].dropna())\ndup_hashes = [h for h, c in dup_counts.items() if c > 1]\n\ndup_df = df[df[\"phash\"].isin(dup_hashes)]\ndup_df.to_csv(REPORT_DIR / \"duplicates.csv\", index=False)\n\n# ---------- Quality Outliers ----------\nlow_b = df[\"brightness\"].quantile(0.05)\nhigh_b = df[\"brightness\"].quantile(0.95)\nlow_s = df[\"sharpness\"].quantile(0.05)\n\noutliers = df[\n    (df[\"brightness\"] < low_b) |\n    (df[\"brightness\"] > high_b) |\n    (df[\"sharpness\"] < low_s)\n]\n\noutliers.to_csv(REPORT_DIR / \"quality_outliers.csv\", index=False)\n\nprint(f\"Duplicate groups: {len(dup_hashes)}\")\nprint(f\"Quality outliers: {len(outliers)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Background Bias Visualization","metadata":{}},{"cell_type":"code","source":"# ---------- Background Bias PCA ----------\nbg_matrix = np.stack(df[\"bg_feats\"].values)\npca = PCA(n_components=2)\nproj = pca.fit_transform(bg_matrix)\n\ndf[\"bg_x\"], df[\"bg_y\"] = proj[:,0], proj[:,1]\n\nplt.figure(figsize=(6,6))\nfor cls in df[\"class\"].unique():\n    subset = df[df[\"class\"] == cls]\n    plt.scatter(subset[\"bg_x\"], subset[\"bg_y\"], s=8, alpha=0.6, label=cls)\n\nplt.legend(bbox_to_anchor=(1.05,1))\nplt.title(\"Background Bias PCA\")\nplt.tight_layout()\nplt.savefig(REPORT_DIR / \"background_bias_pca.png\")\nplt.close()\n\n# Save audit metadata\ndf.drop(columns=[\"bg_feats\"]).to_csv(REPORT_DIR / \"dataset_audit.csv\", index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2. **Manual Review & Cleaning**","metadata":{}},{"cell_type":"markdown","source":"Cell 2 â€” Inspect Audit Outputs","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nfrom IPython.display import display\n\ndisplay(pd.read_csv(REPORT_DIR / \"duplicates.csv\").head())\ndisplay(pd.read_csv(REPORT_DIR / \"quality_outliers.csv\").head())\n\nImage.open(REPORT_DIR / \"background_bias_pca.png\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Cell 3 â€” Remove Duplicate Images (Safe Backup)","metadata":{}},{"cell_type":"code","source":"import shutil\n\nREMOVED_DUP_DIR = Path(\"outputs/removed_duplicates\")\nREMOVED_DUP_DIR.mkdir(parents=True, exist_ok=True)\n\ndup_df = pd.read_csv(REPORT_DIR / \"duplicates.csv\")\n\nfor _, grp in dup_df.groupby(\"phash\"):\n    for _, row in grp.iloc[1:].iterrows():\n        src = DATA_DIR / row[\"split\"] / row[\"class\"] / row[\"filename\"]\n        if src.exists():\n            shutil.move(src, REMOVED_DUP_DIR / row[\"filename\"])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Cell 4 â€” Remove Quality Outliers","metadata":{}},{"cell_type":"code","source":"REMOVED_OUT_DIR = Path(\"outputs/removed_outliers\")\nREMOVED_OUT_DIR.mkdir(parents=True, exist_ok=True)\n\nout_df = pd.read_csv(REPORT_DIR / \"quality_outliers.csv\")\n\nfor _, row in out_df.iterrows():\n    src = DATA_DIR / row[\"split\"] / row[\"class\"] / row[\"filename\"]\n    if src.exists():\n        shutil.move(src, REMOVED_OUT_DIR / row[\"filename\"])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3. **Exploratory Data Analysis (EDA)**","metadata":{}},{"cell_type":"markdown","source":"Cell 5 â€” Class Distribution & Visual Sampling","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport random\n\naudit_df = pd.read_csv(REPORT_DIR / \"dataset_audit.csv\")\n\nplt.figure(figsize=(10,4))\nsns.countplot(data=audit_df, x=\"class\")\nplt.xticks(rotation=90)\nplt.title(\"Class Distribution\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4. **Metadata Preparation**","metadata":{}},{"cell_type":"markdown","source":"Cell 6 â€” Generate Training Metadata","metadata":{}},{"cell_type":"code","source":"metadata = []\n\nfor split in [\"train\", \"val\", \"test\"]:\n    for cls_dir in (DATA_DIR / split).iterdir():\n        if cls_dir.is_dir():\n            for img in cls_dir.iterdir():\n                if img.suffix.lower() in [\".jpg\",\".png\",\".jpeg\"]:\n                    metadata.append({\n                        \"filepath\": str(img),\n                        \"label\": cls_dir.name,\n                        \"split\": split\n                    })\n\nmeta_df = pd.DataFrame(metadata)\nPath(\"outputs/metadata\").mkdir(parents=True, exist_ok=True)\nmeta_df.to_csv(\"outputs/metadata/metadata.csv\", index=False)\n\nprint(\"Metadata saved.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5. **Baseline Model (Frozen Backbone Strategy)**\n\nðŸ“Œ Ideology\n\n* Use pretrained ResNet50\n\n* Freeze backbone â†’ train only classifier\n\n* Establish strong baseline before heavy tuning","metadata":{}},{"cell_type":"markdown","source":"Cell 7 â€” Model Training & Evaluation (Cleaned)","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport timm\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, top_k_accuracy_score\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass PlantDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df.reset_index(drop=True)\n        self.transform = transform\n        self.label2idx = {l:i for i,l in enumerate(sorted(df.label.unique()))}\n        self.idx2label = {v:k for k,v in self.label2idx.items()}\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img = Image.open(row.filepath).convert(\"RGB\")\n        if self.transform:\n            img = self.transform(img)\n        return img, self.label2idx[row.label]\n\n    def __len__(self):\n        return len(self.df)\n\ntransform = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\n\nmeta = pd.read_csv(\"outputs/metadata/metadata.csv\")\ntrain_ds = PlantDataset(meta[meta.split==\"train\"], transform)\nval_ds = PlantDataset(meta[meta.split==\"val\"], transform)\n\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=64)\n\nbackbone = timm.create_model(\"resnet50\", pretrained=True)\nbackbone.reset_classifier(0)\nfor p in backbone.parameters():\n    p.requires_grad = False\n\nmodel = nn.Sequential(\n    backbone,\n    nn.Linear(backbone.num_features, len(train_ds.label2idx))\n).to(DEVICE)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model[1].parameters(), lr=1e-3)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Training Loop","metadata":{}},{"cell_type":"code","source":"def run_epoch(model, loader, train=True):\n    model.train() if train else model.eval()\n    losses, preds, labels, outputs = [], [], [], []\n\n    with torch.set_grad_enabled(train):\n        for x,y in tqdm(loader, leave=False):\n            x,y = x.to(DEVICE), y.to(DEVICE)\n            out = model(x)\n            loss = criterion(out, y)\n\n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n            losses.append(loss.item())\n            preds.append(out.argmax(1).cpu())\n            labels.append(y.cpu())\n            outputs.append(out.cpu())\n\n    preds = torch.cat(preds)\n    labels = torch.cat(labels)\n    outputs = torch.cat(outputs)\n\n    return {\n        \"loss\": np.mean(losses),\n        \"acc\": accuracy_score(labels, preds),\n        \"f1\": f1_score(labels, preds, average=\"weighted\"),\n        \"top3\": top_k_accuracy_score(labels, F.softmax(outputs,1), k=3),\n        \"cm\": confusion_matrix(labels, preds)\n    }\n\nfor epoch in range(3):\n    train_metrics = run_epoch(model, train_loader, True)\n    val_metrics = run_epoch(model, val_loader, False)\n\n    print(f\"Epoch {epoch+1} | \"\n          f\"Train Loss {train_metrics['loss']:.4f} | \"\n          f\"Val Acc {val_metrics['acc']:.4f} | \"\n          f\"F1 {val_metrics['f1']:.4f} | \"\n          f\"Top3 {val_metrics['top3']:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Phase 2: Advanced Training Pipeline (Stage-6 Model)**\nThis phase focuses on robust generalization, domain realism, and stable optimization using modern deep learning practices.","metadata":{}},{"cell_type":"markdown","source":"### 1. **Environment, Reproducibility & Paths**","metadata":{}},{"cell_type":"markdown","source":"Clean Cell â€” Setup","metadata":{}},{"cell_type":"code","source":"# ===========================\n# Environment & Reproducibility\n# ===========================\n\nimport os, random, math\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nimport cv2\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.optim.swa_utils import AveragedModel\nfrom torch.utils.data import Dataset, DataLoader\n\nimport timm\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, top_k_accuracy_score\n\n# ---------------------------\n# Reproducibility\n# ---------------------------\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", DEVICE)\n\n# ---------------------------\n# Paths\n# ---------------------------\nMETADATA_CSV = \"outputs/metadata/metadata.csv\"\n\nOUTPUTS_DIR = Path(\"outputs\")\nCKPT_DIR = OUTPUTS_DIR / \"checkpoints\"\nFIG_DIR  = OUTPUTS_DIR / \"figures\"\nAUG_DIR  = OUTPUTS_DIR / \"augmentation_experiments\"\n\nfor d in [CKPT_DIR, FIG_DIR, AUG_DIR]:\n    d.mkdir(parents=True, exist_ok=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2. **Dataset: Domain-Specific Cleanup + Safety Checks**\nðŸ“Œ Why this matters\n\nIdentified that some classes contained black borders / letterboxing, which:\n\n* Biases the model\n\n* Acts as a shortcut feature\n\n* Reduces real-world performance\n\nConditionally cleaned images per class â€” this is a very strong dataset insight.","metadata":{}},{"cell_type":"markdown","source":"Clean Cell â€” Dataset Class","metadata":{}},{"cell_type":"code","source":"def strip_black_bars_rgb(img, threshold=4):\n    \"\"\"Remove near-black borders if present.\"\"\"\n    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n    col_mean = gray.mean(axis=0)\n    row_mean = gray.mean(axis=1)\n\n    def bounds(arr):\n        lo = next((i for i, v in enumerate(arr) if v > threshold), 0)\n        hi = len(arr) - next((i for i, v in enumerate(arr[::-1]) if v > threshold), 1)\n        return lo, hi\n\n    x0, x1 = bounds(col_mean)\n    y0, y1 = bounds(row_mean)\n\n    if x1 <= x0 or y1 <= y0:\n        return img\n    return img[y0:y1, x0:x1]\n\n\nclass PlantDataset(Dataset):\n    def __init__(self, df, transform=None, label2idx=None):\n        self.df = df.reset_index(drop=True)\n        self.transform = transform\n\n        if label2idx is None:\n            labels = sorted(self.df[\"label\"].unique())\n            self.label2idx = {l: i for i, l in enumerate(labels)}\n        else:\n            self.label2idx = label2idx\n\n        self.idx2label = {v: k for k, v in self.label2idx.items()}\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img = cv2.imread(row[\"filepath\"])\n        if img is None:\n            raise FileNotFoundError(row[\"filepath\"])\n\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        # Class-conditional cleanup\n        lname = row[\"label\"].lower()\n        if \"cucumber\" in lname or \"corn_common_rust\" in lname:\n            img = strip_black_bars_rgb(img)\n\n        if self.transform:\n            img = self.transform(image=img)[\"image\"]\n\n        return img, self.label2idx[row[\"label\"]]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3. **Domain-Realistic Augmentations (Very Important)**\nðŸ“Œ Ideology behind these augmentations\n\nDid not use random extreme transforms.\n\nInstead, simulated:\n\n* Camera variation\n\n*  Lighting variation\n\n* Compression artifacts\n\n* Minor blur / motion","metadata":{}},{"cell_type":"markdown","source":"Augmentations","metadata":{}},{"cell_type":"code","source":"IM_SIZE = 320\nIM_MEAN = [0.485, 0.456, 0.406]\nIM_STD  = [0.229, 0.224, 0.225]\n\ndef get_train_transform():\n    return A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.RandomRotate90(p=0.5),\n\n        A.RandomResizedCrop(\n            height=IM_SIZE, width=IM_SIZE,\n            scale=(0.8, 1.0), ratio=(0.8, 1.25), p=1.0\n        ),\n\n        A.ColorJitter(0.2, 0.2, 0.2, 0.05, p=0.5),\n        A.OneOf([\n            A.GaussianBlur(blur_limit=(3,5)),\n            A.MotionBlur(blur_limit=5),\n        ], p=0.25),\n\n        A.GaussNoise(var_limit=(5,25), p=0.25),\n        A.RandomGamma((80,120), p=0.3),\n        A.ImageCompression(40, 90, p=0.3),\n\n        A.Normalize(IM_MEAN, IM_STD),\n        ToTensorV2()\n    ])\n\ndef get_val_transform():\n    return A.Compose([\n        A.LongestMaxSize(IM_SIZE),\n        A.PadIfNeeded(IM_SIZE, IM_SIZE, border_mode=cv2.BORDER_CONSTANT),\n        A.CenterCrop(IM_SIZE, IM_SIZE),\n        A.Normalize(IM_MEAN, IM_STD),\n        ToTensorV2()\n    ])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Loss & Model","metadata":{}},{"cell_type":"code","source":"class LabelSmoothingCE(nn.Module):\n    def __init__(self, eps=0.1):\n        super().__init__()\n        self.eps = eps\n\n    def forward(self, logits, target):\n        n = logits.size(1)\n        log_probs = F.log_softmax(logits, dim=1)\n\n        with torch.no_grad():\n            smooth = torch.full_like(log_probs, self.eps / (n - 1))\n            smooth.scatter_(1, target.unsqueeze(1), 1 - self.eps)\n\n        return torch.mean(torch.sum(-smooth * log_probs, dim=1))\n\n\ndef create_model(num_classes):\n    return timm.create_model(\n        \"efficientnet_b3\",\n        pretrained=True,\n        num_classes=num_classes\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4. **Training & Validation Engine (AMP + Clipping)**","metadata":{}},{"cell_type":"code","source":"def train_one_epoch(model, loader, optimizer, scaler, criterion, scheduler=None):\n    model.train()\n    total_loss = 0\n\n    for x, y in tqdm(loader, leave=False):\n        x, y = x.to(DEVICE), y.to(DEVICE)\n        optimizer.zero_grad(set_to_none=True)\n\n        with autocast(enabled=DEVICE.type == \"cuda\"):\n            logits = model(x)\n            loss = criterion(logits, y)\n\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        scaler.step(optimizer)\n        scaler.update()\n\n        if scheduler:\n            scheduler.step()\n\n        total_loss += loss.item() * x.size(0)\n\n    return total_loss / len(loader.dataset)\n\n\n@torch.no_grad()\ndef validate(model, loader, criterion):\n    model.eval()\n    total_loss, preds, labels, logits_all = 0, [], [], []\n\n    for x, y in tqdm(loader, leave=False):\n        x, y = x.to(DEVICE), y.to(DEVICE)\n        with autocast(enabled=DEVICE.type == \"cuda\"):\n            logits = model(x)\n            loss = criterion(logits, y)\n\n        total_loss += loss.item() * x.size(0)\n        preds.append(logits.argmax(1).cpu().numpy())\n        labels.append(y.cpu().numpy())\n        logits_all.append(logits.cpu())\n\n    preds = np.concatenate(preds)\n    labels = np.concatenate(labels)\n    logits_all = torch.cat(logits_all)\n\n    probs = F.softmax(logits_all, dim=1).numpy()\n\n    return {\n        \"loss\": total_loss / len(loader.dataset),\n        \"acc\": accuracy_score(labels, preds),\n        \"f1\": f1_score(labels, preds, average=\"weighted\"),\n        \"top3\": top_k_accuracy_score(labels, probs, k=3),\n        \"cm\": confusion_matrix(labels, preds)\n    }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5. **Stage-6 Training Strategy**\n\n**Phase A â€“ Warm-up**\n\n* Freeze backbone\n\n* Train classifier head only\n\n* Prevents destroying pretrained features\n\n**Phase B â€“ Fine-tuning**\n\n* Unfreeze backbone\n\n* Discriminative LR (head > backbone)\n\n* OneCycleLR\n\n* Early stopping","metadata":{}},{"cell_type":"code","source":"def train_stage6(train_df, val_df,\n                 batch_size=32,\n                 warmup_epochs=2,\n                 finetune_epochs=8):\n\n    tf_train, tf_val = get_train_transform(), get_val_transform()\n\n    train_ds = PlantDataset(train_df, tf_train)\n    val_ds   = PlantDataset(val_df, tf_val, train_ds.label2idx)\n\n    train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=2)\n    val_loader   = DataLoader(val_ds, batch_size*2, shuffle=False, num_workers=2)\n\n    model = create_model(len(train_ds.label2idx)).to(DEVICE)\n    criterion = LabelSmoothingCE(0.1)\n    scaler = GradScaler()\n\n    # Phase A: head only\n    for p in model.parameters(): p.requires_grad = False\n    for p in model.classifier.parameters(): p.requires_grad = True\n\n    optimizer = optim.AdamW(model.classifier.parameters(), lr=3e-4, weight_decay=1e-2)\n    scheduler = optim.lr_scheduler.OneCycleLR(\n        optimizer, max_lr=3e-4,\n        total_steps=len(train_loader)*warmup_epochs\n    )\n\n    best_loss = float(\"inf\")\n    best_state = None\n\n    print(\"Phase A: Head warm-up\")\n    for _ in range(warmup_epochs):\n        train_one_epoch(model, train_loader, optimizer, scaler, criterion, scheduler)\n        metrics = validate(model, val_loader, criterion)\n\n        if metrics[\"loss\"] < best_loss:\n            best_loss = metrics[\"loss\"]\n            best_state = model.state_dict()\n\n    # Phase B: full fine-tuning\n    for p in model.parameters(): p.requires_grad = True\n\n    optimizer = optim.AdamW([\n        {\"params\": model.classifier.parameters(), \"lr\": 3e-4},\n        {\"params\": [p for n,p in model.named_parameters() if \"classifier\" not in n], \"lr\": 1e-4}\n    ], weight_decay=1e-2)\n\n    scheduler = optim.lr_scheduler.OneCycleLR(\n        optimizer, max_lr=3e-4,\n        total_steps=len(train_loader)*finetune_epochs\n    )\n\n    print(\"Phase B: Fine-tuning\")\n    for _ in range(finetune_epochs):\n        train_one_epoch(model, train_loader, optimizer, scaler, criterion, scheduler)\n        metrics = validate(model, val_loader, criterion)\n\n        if metrics[\"loss\"] < best_loss:\n            best_loss = metrics[\"loss\"]\n            best_state = model.state_dict()\n\n    model.load_state_dict(best_state)\n    torch.save(best_state, CKPT_DIR / \"effb3_stage6.pt\")\n\n    return metrics\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" # **Phase 3: Robustness & Generalization Training**\nðŸŽ¯ Goal of This Phase\n\nAfter achieving strong validation performance with Stage-6 EfficientNet-B3, the goal of this phase is to:\n\n* Reduce overfitting to clean/lab images\n\n* Improve decision boundary smoothness\n\n* Force the model to rely on leaf texture & disease patterns, not background cues\n\n* Test whether performance remains stable under harder perturbations\n\nThis phase is not about chasing accuracy, but about trustworthy generalization.","metadata":{}},{"cell_type":"markdown","source":"Cell A â€” Stronger Domain Augmentations\n\nTo simulate:\n\n* phone cameras\n\n* uneven sunlight\n\n* fog/dust\n\n* compression artifacts","metadata":{}},{"cell_type":"code","source":"IM_SIZE = 320\nIM_MEAN = [0.485, 0.456, 0.406]\nIM_STD  = [0.229, 0.224, 0.225]\n\ndef get_train_transform_stronger():\n    return A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.RandomRotate90(p=0.5),\n\n        A.RandomResizedCrop(\n            height=IM_SIZE, width=IM_SIZE,\n            scale=(0.8, 1.0), ratio=(0.8, 1.25), p=1.0\n        ),\n\n        A.ColorJitter(0.25, 0.25, 0.25, 0.07, p=0.6),\n        A.OneOf([\n            A.GaussianBlur(blur_limit=(3,7)),\n            A.MotionBlur(blur_limit=7),\n        ], p=0.35),\n\n        A.GaussNoise(var_limit=(5,35), p=0.35),\n        A.RandomShadow(p=0.3),\n        A.RandomFog(0.1, 0.3, alpha_coef=0.08, p=0.2),\n\n        A.RandomGamma((70,130), p=0.5),\n        A.RandomBrightnessContrast(0.25, 0.25, p=0.5),\n        A.ImageCompression(25, 90, p=0.6),\n\n        A.Normalize(IM_MEAN, IM_STD),\n        ToTensorV2(),\n    ])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Cell B â€” Hardened Augmentations (Bias Breaking)","metadata":{}},{"cell_type":"code","source":"def get_train_transform_hardened():\n    return A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.2),\n        A.RandomRotate90(p=0.4),\n\n        # Leaf-focused crops\n        A.OneOf([\n            A.RandomResizedCrop(IM_SIZE, IM_SIZE, scale=(0.85,1.0), ratio=(0.9,1.1)),\n            A.RandomResizedCrop(IM_SIZE, IM_SIZE, scale=(0.8,1.0),  ratio=(0.8,1.25)),\n        ], p=1.0),\n\n        A.ColorJitter(0.25,0.25,0.25,0.07,p=0.6),\n        A.OneOf([\n            A.GaussianBlur((3,7)),\n            A.MotionBlur(7),\n        ], p=0.35),\n\n        A.GaussNoise((5,35), p=0.3),\n\n        # Break black-border shortcut learning\n        A.OneOf([\n            A.Compose([\n                A.SmallestMaxSize(IM_SIZE),\n                A.PadIfNeeded(IM_SIZE, IM_SIZE, border_mode=cv2.BORDER_CONSTANT, value=(128,128,128)),\n                A.CenterCrop(IM_SIZE, IM_SIZE),\n            ]),\n            A.NoOp(),\n        ], p=0.2),\n\n        A.RandomGamma((70,130), p=0.5),\n        A.RandomBrightnessContrast(0.25,0.25,p=0.5),\n        A.ImageCompression(25,90,p=0.5),\n\n        # Explicit background reliance reduction\n        A.CoarseDropout(\n            max_holes=6,\n            max_height=IM_SIZE//8,\n            max_width=IM_SIZE//8,\n            fill_value=0,\n            p=0.2\n        ),\n\n        A.Normalize(IM_MEAN, IM_STD),\n        ToTensorV2(),\n    ])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" MixUp & CutMix (Sample-Level Regularization)\n\n* Stage-6 already learned strong features\n\n* MixUp/CutMix now:\n\n    * smooth class boundaries\n\n    * reduce memorization\n\n    * improve minority class behavior","metadata":{}},{"cell_type":"markdown","source":"Cell C â€” MixUp / CutMix Utilities","metadata":{}},{"cell_type":"code","source":"def sample_lambda(alpha):\n    return float(np.random.beta(alpha, alpha)) if alpha > 0 else 1.0\n\ndef mixup(x, y, lam):\n    idx = torch.randperm(x.size(0), device=x.device)\n    return lam*x + (1-lam)*x[idx], y, y[idx], lam\n\ndef cutmix(x, y, lam):\n    B, C, H, W = x.shape\n    idx = torch.randperm(B, device=x.device)\n\n    rw, rh = int(W*np.sqrt(1-lam)), int(H*np.sqrt(1-lam))\n    cx, cy = np.random.randint(W), np.random.randint(H)\n\n    x1, y1 = max(cx-rw//2,0), max(cy-rh//2,0)\n    x2, y2 = min(cx+rw//2,W), min(cy+rh//2,H)\n\n    x[:, :, y1:y2, x1:x2] = x[idx, :, y1:y2, x1:x2]\n    lam = 1 - ((x2-x1)*(y2-y1))/(W*H)\n    return x, y, y[idx], lam\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Training Loop with MixUp / CutMix","metadata":{}},{"cell_type":"code","source":"def train_one_epoch_mix(\n    model, loader, optimizer, scaler, device,\n    mixup_alpha=0.2, cutmix_alpha=0.2, ls_eps=0.1\n):\n    model.train()\n    total = 0.0\n\n    for x, y in tqdm(loader, leave=False):\n        x, y = x.to(device), y.to(device)\n\n        use_mix = mixup_alpha > 0 and np.random.rand() < 0.5\n        lam = sample_lambda(mixup_alpha if use_mix else cutmix_alpha)\n\n        if use_mix:\n            x, ya, yb, lam = mixup(x, y, lam)\n        else:\n            x, ya, yb, lam = cutmix(x, y, lam)\n\n        optimizer.zero_grad(set_to_none=True)\n\n        with torch.amp.autocast(device_type=\"cuda\", enabled=device.type==\"cuda\"):\n            logits = model(x)\n            loss = _soft_target_ce(logits, ya, yb, lam, eps=ls_eps)\n\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        scaler.step(optimizer)\n        scaler.update()\n\n        total += loss.item() * x.size(0)\n\n    return total / len(loader.dataset)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Final Fine-Tuning with Early Stopping","metadata":{}},{"cell_type":"code","source":"def finetune_with_early_stopping(\n    train_df, val_df,\n    resume_ckpt,\n    save_name,\n    max_epochs=15,\n    patience=3,\n    batch_size=32,\n    mixup_alpha=0.3,\n    cutmix_alpha=0.3\n):\n    tf_train = get_train_transform_stronger()\n    tf_val   = get_val_transform()\n\n    tmp = PlantDataset(train_df, tf_val)\n    label2idx = tmp.label2idx\n\n    train_ds = PlantDataset(train_df, tf_train, label2idx)\n    val_ds   = PlantDataset(val_df,   tf_val,   label2idx)\n\n    train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n    val_loader   = DataLoader(val_ds, batch_size*2)\n\n    model = create_efficientnet_b3(len(label2idx)).to(DEVICE)\n    model.load_state_dict(torch.load(resume_ckpt)[\"model\"])\n\n    optimizer = torch.optim.AdamW(\n        get_param_groups(model, 1.5e-4, 5e-5, 1e-2)\n    )\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n    criterion = LabelSmoothingCE(0.1)\n    scaler = torch.amp.GradScaler(\"cuda\")\n\n    best_loss, wait = float(\"inf\"), 0\n    best_state = None\n\n    for ep in range(max_epochs):\n        tr = train_one_epoch_mix(\n            model, train_loader, optimizer, scaler, DEVICE,\n            mixup_alpha=mixup_alpha, cutmix_alpha=cutmix_alpha\n        )\n        va_loss, va_acc, va_f1, va_top3, va_cm = validate(model, val_loader, criterion, DEVICE)\n        scheduler.step()\n\n        print(f\"[FT {ep+1}] train {tr:.4f} | val {va_loss:.4f}\")\n\n        if va_loss < best_loss:\n            best_loss, wait = va_loss, 0\n            best_state = {\"model\": model.state_dict(), \"label2idx\": label2idx}\n        else:\n            wait += 1\n            if wait >= patience:\n                print(\"Early stopping.\")\n                break\n\n    out = CKPT_DIR / save_name\n    torch.save(best_state, out)\n    return out\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}