{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31239,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **Methodology: Curriculum-Driven Robust Fine-Tuning**\n\n### **Problem**\n\nSome plant disease classes still failed due to:\n\n* visually-similar lesions across species\n* long-tail labels with weak supervision\n* Grad-CAM showing attention on background instead of lesions\n\n---\n\n### **Step-1: Diagnose**\n\nWe computed:\n\n* per-class F1\n* confusion matrix\n* Grad-CAM lesion overlap\n\nThis revealed *where* the model was looking and *why* it failed.\n\n---\n\n### **Step-2: Counterfactual edits**\n\nFor misclassified images, we applied edits like:\n\n* sharpening\n* center cropping\n* contrast/saturation tweaks\n* jpeg degradation\n\nFor each edit we measured:\n\n> change in probability margin between the true and confused class.\n\nEdits that improved the margin became **per-class recommendations**.\n\n---\n\n### **Step-3: Build curriculum**\n\nFrom these results we generated:\n\n| File                   | Purpose                          |\n| ---------------------- | -------------------------------- |\n| `aug_map.json`         | label-specific augmentations     |\n| `w_label.json`         | sampling weights for weak labels |\n| `alpha_per_class.json` | focal loss emphasis              |\n\nThis forces the model to **practice cases it previously failed**.\n\n---\n\n### **Step-4: Fine-Tune**\n\nTwo-stage training:\n\n1️⃣ backbone low LR + head higher LR\n2️⃣ short correction epoch → stabilizes without overfitting\n\n---\n\n### **Step-5: Validate**\n\nWe recomputed metrics and overlap deltas.\nImprovements concentrated exactly on:\n\n* overlapping lesion confusions\n* low-F1 classes\n* CAM shifting toward lesion regions\n\n---\n\n### **Result**\n\nThe curriculum delivered **targeted accuracy gains** without harming global performance — and interpretability confirmed behavior improved rather than just numbers.","metadata":{}},{"cell_type":"markdown","source":"Setup & metadata","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# ===========================\n# Section 1 — Setup & metadata\n# ===========================\nimport os, json, math, cv2, timm, torch\nimport numpy as np, pandas as pd\nfrom pathlib import Path\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nIM_SIZE = 320\nIM_MEAN = [0.485, 0.456, 0.406]\nIM_STD  = [0.229, 0.224, 0.225]\n\nprint(\"Device:\", DEVICE)\n\n# ---- paths ----\nBASE_DIR = Path(\"/kaggle/working/v6_global_eval\")\nBASE_DIR.mkdir(parents=True, exist_ok=True)\n\nTRAIN_CSV = \"/kaggle/input/k/adiithape/cnn-model-v3/outputs/metadata/curated_train_v6.csv\"\nVAL_CSV   = \"/kaggle/input/k/adiithape/cnn-model-v3/outputs/metadata/curated_val_v6.csv\"\nMAP_JSON  = \"/kaggle/input/k/adiithape/cnn-model-v3/outputs/metadata/label2idx_v6.json\"\n\nCKPT_PATH = \"/kaggle/input/v5-model/effb3_320_curated_no_cotton_best_v5.pt\"\n\nTEMP = 0.551  # current calibration temperature\n\ntrain_df = pd.read_csv(TRAIN_CSV)\nval_df   = pd.read_csv(VAL_CSV)\n\nwith open(MAP_JSON,\"r\") as f:\n    label2idx = json.load(f)\n\nidx2label = {i:l for l,i in label2idx.items()}\nclasses = [idx2label[i] for i in range(len(idx2label))]\n\nprint(\"Classes:\", len(classes))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Load model + preprocessing","metadata":{}},{"cell_type":"code","source":"# ===========================\n# Section 2 — Model + preprocessing\n# ===========================\nmodel = timm.create_model(\"efficientnet_b3\", pretrained=False, num_classes=len(classes))\nstate = torch.load(CKPT_PATH, map_location=\"cpu\")\nif isinstance(state, dict) and \"model\" in state:\n    model.load_state_dict(state[\"model\"], strict=True)\nelse:\n    model.load_state_dict(state, strict=True)\n\nmodel = model.to(DEVICE).eval()\n\ndef load_rgb(path):\n    img = cv2.imread(str(path))\n    if img is None: raise FileNotFoundError(path)\n    return img\n\ndef preprocess_bgr_pad(img):\n    h,w = img.shape[:2]\n    s = IM_SIZE/max(h,w)\n    nh,nw = int(h*s), int(w*s)\n    img = cv2.resize(img,(nw,nh), cv2.INTER_AREA)\n    canvas = np.zeros((IM_SIZE,IM_SIZE,3), dtype=img.dtype)\n    y0 = (IM_SIZE-nh)//2; x0 = (IM_SIZE-nw)//2\n    canvas[y0:y0+nh,x0:x0+nw] = img\n    x = (canvas[:,:,::-1]/255.0 - IM_MEAN)/IM_STD\n    return torch.from_numpy(np.transpose(x,(2,0,1))).float()\n\ndef preprocess_bgr_center_crop(img):\n    h,w = img.shape[:2]\n    if h < w: nh,nw = IM_SIZE, int(w*IM_SIZE/h)\n    else:     nh,nw = int(h*IM_SIZE/w), IM_SIZE\n    img = cv2.resize(img,(nw,nh), cv2.INTER_AREA)\n    y0 = (nh-IM_SIZE)//2; x0 = (nw-IM_SIZE)//2\n    img = img[y0:y0+IM_SIZE, x0:x0+IM_SIZE]\n    x = (img[:,:,::-1]/255.0 - IM_MEAN)/IM_STD\n    return torch.from_numpy(np.transpose(x,(2,0,1))).float()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Global inference cache (validation + Google test)","metadata":{}},{"cell_type":"code","source":"# ===========================\n# Section 3 — Global inference cache\n# ===========================\n@torch.no_grad()\ndef predict_dual_logits(img, T=TEMP):\n    x1 = preprocess_bgr_pad(img).unsqueeze(0).to(DEVICE)\n    x2 = preprocess_bgr_center_crop(img).unsqueeze(0).to(DEVICE)\n    z = (model(x1)+model(x2))/2\n    if T: z = z/float(T)\n    return z.squeeze(0).cpu()\n\ndef run_infer_cache(df, out_csv):\n    rows = []\n    for i,r in df.iterrows():\n        img = load_rgb(r.filepath)\n        z = predict_dual_logits(img)\n        p = F.softmax(z,dim=0).numpy()\n        top = p.argsort()[::-1][:3]\n        rows.append({\n            \"filepath\": r.filepath,\n            \"gt\": r.label,\n            \"pred\": classes[top[0]],\n            \"prob\": float(p[top[0]]),\n            \"pred2\": classes[top[1]],\n            \"pred3\": classes[top[2]]\n        })\n    pd.DataFrame(rows).to_csv(out_csv, index=False)\n    print(\"Saved:\", out_csv)\n\nOUT = BASE_DIR/\"preds_v6_val_dualT.csv\"\nrun_infer_cache(val_df[[\"filepath\",\"label\"]], OUT)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"CAM lesion-overlap analysis","metadata":{}},{"cell_type":"code","source":"# ===========================\n# Section 4 — CAM lesion alignment\n# ===========================\nimport matplotlib.pyplot as plt\nfrom torch import nn\n\nPRED_CSV = BASE_DIR/\"preds_v6_val_dualT.csv\"\npreds = pd.read_csv(PRED_CSV)\ncls2idx = {c:i for i,c in enumerate(classes)}\n\ntarget_layer = model.conv_head\n\ndef gradcam(img, cls):\n    acts, grads = [],[]\n    def f(m,i,o): acts.append(o.detach())\n    def b(m,gi,go): grads.append(go[0].detach())\n    h1 = target_layer.register_forward_hook(f)\n    h2 = target_layer.register_full_backward_hook(b)\n\n    x = preprocess_bgr_center_crop(img).unsqueeze(0).to(DEVICE)\n    s = model(x)[0,cls]; model.zero_grad(); s.backward()\n\n    a, g = acts[-1][0], grads[-1][0]\n    w = g.mean((1,2),keepdim=True)\n    cam = (w*a).sum(0).cpu().numpy()\n    cam = (cam-cam.min())/(cam.max()+1e-6)\n\n    h1.remove(); h2.remove()\n    return cv2.resize(cam,(IM_SIZE,IM_SIZE))\n\ndef lesion_mask(img):\n    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)[:,:,0]\n    lab = cv2.createCLAHE(2.0,(8,8)).apply(lab)\n    e = cv2.Canny(lab,60,120)\n    e = cv2.dilate(e,np.ones((3,3),np.uint8))\n    return cv2.resize((e>0).astype(np.uint8),(IM_SIZE,IM_SIZE))\n\nrows=[]\nfor _,r in preds.iterrows():\n    img = load_rgb(r.filepath)\n    cls = cls2idx[r.gt]\n    cam = gradcam(img,cls)\n    mask = lesion_mask(img)\n    ov = float((cam*(mask>0)).sum()/(cam.sum()+1e-6))\n    rows.append({**r.to_dict(),\"overlap\":ov})\ndf_cam = pd.DataFrame(rows)\ndf_cam.to_csv(BASE_DIR/\"cam_overlap_all_species.csv\", index=False)\nprint(\"Saved CAM overlap CSV\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Counterfactual edits → deltas","metadata":{}},{"cell_type":"code","source":"# ===========================\n# Section 5 — Counterfactual edits\n# ===========================\nedits = {\n    \"center_bias\": lambda im: cv2.resize(im[int(0.1*im.shape[0]):int(0.9*im.shape[0]),\n                                            int(0.1*im.shape[1]):int(0.9*im.shape[1])], (im.shape[1],im.shape[0])),\n    \"sharpen\": lambda im: cv2.addWeighted(im,1.5,cv2.GaussianBlur(im,(0,0),1.0),-0.5,0),\n    \"contrast+10%\": lambda im: cv2.convertScaleAbs(im,alpha=1.1,beta=0),\n    \"bright+10\": lambda im: cv2.convertScaleAbs(im,alpha=1.0,beta=10),\n}\n\npair = df_cam.copy()\npair[\"correct\"]=pair[\"gt\"]==pair[\"pred\"]\n\nrows=[]\nfor _,r in pair.sample(300, random_state=42).iterrows():  # cap\n    img = load_rgb(r.filepath)\n    p0 = F.softmax(predict_dual_logits(img),dim=0).numpy()\n    t = cls2idx[r.gt]; pw = cls2idx[r.pred]\n    for name,fn in edits.items():\n        im1 = fn(img)\n        p1 = F.softmax(predict_dual_logits(im1),dim=0).numpy()\n        rows.append({\n            \"true\":r.gt,\"wrong\":r.pred,\"edit\":name,\n            \"d_margin\": (p1[t]-p1[pw]) - (p0[t]-p0[pw])\n        })\ndf_delta = pd.DataFrame(rows)\ndf_delta.to_csv(BASE_DIR/\"counterfactual_deltas.csv\", index=False)\nprint(\"Saved deltas\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Build class-specific augmentation map","metadata":{}},{"cell_type":"code","source":"# ===========================\n# Section 6 — Build class augmentation map\n# ===========================\nwinners = (\n    df_delta.groupby([\"true\",\"edit\"])[\"d_margin\"]\n    .mean().reset_index()\n    .sort_values([\"true\",\"d_margin\"],ascending=[True,False])\n)\n\naug_map={}\nfor lbl,g in winners.groupby(\"true\"):\n    names=g.head(3)[\"edit\"].tolist()\n    cfg=dict(center_crop_p=0.1, blur_p=0.1, sharpen_p=0, color_jitter_p=0.08,\n             cj_brightness=0.1,cj_contrast=0.1,cj_saturation=0.1,cj_hue=0.02)\n    if \"center_bias\" in names: cfg[\"center_crop_p\"]=0.35\n    if \"sharpen\" in names: cfg[\"sharpen_p\"]=0.25\n    if \"contrast\" in names: cfg[\"cj_contrast\"]=0.14\n    aug_map[lbl]=cfg\n\n(AUG := BASE_DIR/\"curriculum/aug_map.json\").parent.mkdir(parents=True, exist_ok=True)\njson.dump(aug_map, open(AUG,\"w\"), indent=2)\nprint(\"Saved\", AUG)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Curriculum label weights & alpha (hard classes get focal loss)","metadata":{}},{"cell_type":"code","source":"# ===========================\n# Section 7 — Curriculum & alpha\n# ===========================\ncam = pd.read_csv(BASE_DIR/\"cam_overlap_all_species.csv\")\nlow = cam.groupby(\"gt\")[\"overlap\"].mean().sort_values().head(30)\n\nw_label={c:1.0 for c in classes}\nfor i,(lbl,val) in enumerate(low.items()):\n    w_label[lbl]=1.0+0.6*(1-i/max(1,len(low)-1))\n\nalpha_per_class={label2idx[l]:1.35 for l in low.head(10).index}\n\njson.dump(w_label, open(BASE_DIR/\"curriculum/w_label.json\",\"w\"), indent=2)\njson.dump(alpha_per_class, open(BASE_DIR/\"curriculum/alpha_per_class.json\",\"w\"), indent=2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Unified fine-tuning (final)","metadata":{}},{"cell_type":"code","source":"# fine_tune_v6_unified.py\nimport os, json, cv2, math, torch, timm, numpy as np, pandas as pd\nfrom pathlib import Path\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# ---- Config/paths ----\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nIM_SIZE = 320\nIM_MEAN = [0.485, 0.456, 0.406]; IM_STD = [0.229, 0.224, 0.225]\nCKPT_IN = \"/kaggle/input/v5-model/effb3_320_curated_no_cotton_best_v5.pt\"\nCKPT_OUT = \"/kaggle/working/v6_global_eval/effb3_v6_unified_ft.pt\"\nMAP_JSON = \"/kaggle/input/k/adiithape/cnn-model-v3/outputs/metadata/label2idx_v6.json\"\nTRAIN_CSV = \"/kaggle/input/k/adiithape/cnn-model-v3/outputs/metadata/curated_train_v6.csv\"\nVAL_CSV   = \"/kaggle/input/k/adiithape/cnn-model-v3/outputs/metadata/curated_val_v6.csv\"\n\nCURR_DIR = Path(\"/kaggle/working/v6_global_eval/curriculum\")\nAUGMAP_JSON = CURR_DIR/\"aug_map.json\"\nW_LABEL_JSON = CURR_DIR/\"w_label.json\"\nALPHA_JSON   = CURR_DIR/\"alpha_per_class.json\"\n\nEPOCHS = 2\nBS_TRAIN = 32\nBS_VAL = 64\nLR_BACKBONE = 1e-5\nLR_HEAD = 3e-5\nWD = 1e-2\nTEMP = 0.551\n\n# ---- Load metadata and curriculum ----\ntrain_df = pd.read_csv(TRAIN_CSV)\nval_df   = pd.read_csv(VAL_CSV)\nwith open(MAP_JSON, \"r\") as f: label2idx = json.load(f)\nidx2label = {int(v):k for k,v in label2idx.items()}\nclasses = [idx2label[i] for i in range(len(idx2label))]\n\nwith open(AUGMAP_JSON,\"r\") as f: aug_map = json.load(f)\nwith open(W_LABEL_JSON,\"r\") as f: w_label = json.load(f)\nalpha_per_class = {}\nif ALPHA_JSON.exists():\n    with open(ALPHA_JSON,\"r\") as f: alpha_per_class = {int(k):float(v) for k,v in json.load(f).items()}\n\n# ---- Aug helpers ----\ndef sharpen_unsharp(img):\n    blur = cv2.GaussianBlur(img, (0,0), 1.0)\n    return cv2.addWeighted(img, 1.5, blur, -0.5, 0)\n\ndef sharpen_unsharp_aug(img, **kwargs):\n    # Albumentations passes extra params; ignore them\n    return sharpen_unsharp(img)\n\ndef make_transform_for_label(lbl: str):\n    cfg = aug_map.get(lbl, {\n        \"center_crop_p\": 0.10, \"blur_p\": 0.10, \"sharpen_p\": 0.00,\n        \"color_jitter_p\": 0.08, \"cj_brightness\":0.10, \"cj_contrast\":0.10, \"cj_saturation\":0.10, \"cj_hue\":0.02,\n    })\n    cfg.setdefault(\"center_crop_p\", 0.10)\n    cfg.setdefault(\"blur_p\", 0.10)\n    cfg.setdefault(\"sharpen_p\", 0.00)\n    cfg.setdefault(\"color_jitter_p\", 0.08)\n    cfg.setdefault(\"cj_brightness\", 0.10)\n    cfg.setdefault(\"cj_contrast\", 0.10)\n    cfg.setdefault(\"cj_saturation\", 0.10)\n    cfg.setdefault(\"cj_hue\", 0.02)\n\n    tfs = []\n    if cfg[\"center_crop_p\"] > 0:\n        tfs.append(\n            A.RandomResizedCrop(\n                size=(IM_SIZE, IM_SIZE),\n                scale=(0.85, 1.0),\n                ratio=(0.9, 1.1),\n                interpolation=cv2.INTER_AREA,\n                p=cfg[\"center_crop_p\"],\n            )\n        )\n    else:\n        tfs.append(A.Resize(IM_SIZE, IM_SIZE, interpolation=cv2.INTER_AREA))\n\n    tfs.append(A.HorizontalFlip(p=0.5))\n    tfs.append(A.ShiftScaleRotate(shift_limit=0.02, scale_limit=0.05, rotate_limit=15,\n                                  border_mode=cv2.BORDER_REFLECT_101, p=0.35))\n\n    if cfg[\"color_jitter_p\"] > 0:\n        tfs.append(A.ColorJitter(brightness=cfg[\"cj_brightness\"], contrast=cfg[\"cj_contrast\"],\n                                 saturation=cfg[\"cj_saturation\"], hue=cfg[\"cj_hue\"],\n                                 p=cfg[\"color_jitter_p\"]))\n\n    if cfg[\"blur_p\"] > 0:\n        tfs.append(A.GaussianBlur(blur_limit=3, p=cfg[\"blur_p\"]))\n\n    if cfg[\"sharpen_p\"] > 0:\n        tfs.append(A.Lambda(image=sharpen_unsharp_aug, p=cfg[\"sharpen_p\"]))\n\n    tfs.append(A.ImageCompression(quality_lower=70, quality_upper=95, p=0.15))\n    tfs.append(A.Normalize(mean=IM_MEAN, std=IM_STD))\n    tfs.append(ToTensorV2())\n    return A.Compose(tfs)\n\n\n# ---- Dataset ----\nclass PlantDS(Dataset):\n    def __init__(self, df):\n        self.df = df.reset_index(drop=True)\n    def __len__(self): return len(self.df)\n    def __getitem__(self, i):\n        r = self.df.iloc[i]\n        lbl = r[\"label\"]\n        img = cv2.imread(r[\"filepath\"], cv2.IMREAD_COLOR)\n        if img is None:\n            img = np.zeros((IM_SIZE, IM_SIZE, 3), np.uint8)\n        tf = make_transform_for_label(lbl)\n        x = tf(image=img)[\"image\"]\n        y = label2idx[lbl]\n        return x, y\n\n# ---- Sampler ----\ndef build_weights(df):\n    names = df[\"filepath\"].astype(str).str.lower()\n    w = np.ones(len(df), dtype=np.float32)\n    for i, (fp, lbl) in enumerate(zip(df[\"filepath\"], df[\"label\"])):\n        w[i] *= float(w_label.get(lbl, 1.0))\n        # slight ambiguity nudge\n        if any(k in fp.lower() for k in [\"spot\",\"blight\",\"rust\",\"mildew\",\"mosaic\",\"canker\"]):\n            w[i] *= 1.05\n    return w\n\nw = build_weights(train_df)\nsampler = WeightedRandomSampler(w, num_samples=len(w), replacement=True)\n\ntrain_loader = DataLoader(PlantDS(train_df), batch_size=BS_TRAIN, sampler=sampler, num_workers=2, pin_memory=True)\nval_loader   = DataLoader(PlantDS(val_df),   batch_size=BS_VAL, shuffle=False, num_workers=2, pin_memory=True)\n\n# ---- Model ----\nnum_classes = len(classes)\nmodel = timm.create_model(\"efficientnet_b3\", pretrained=False, num_classes=num_classes).to(DEVICE)\nstate = torch.load(CKPT_IN, map_location=\"cpu\")\nif isinstance(state, dict) and \"model\" in state and isinstance(state[\"model\"], dict):\n    model.load_state_dict(state[\"model\"], strict=True)\nelse:\n    model.load_state_dict(state, strict=True)\n\n# param groups\nbackbone_params, head_params = [], []\nfor n,p in model.named_parameters():\n    if not p.requires_grad: continue\n    if \"classifier\" in n: head_params.append(p)\n    else: backbone_params.append(p)\nopt = torch.optim.AdamW([\n    {\"params\": backbone_params, \"lr\": LR_BACKBONE},\n    {\"params\": head_params, \"lr\": LR_HEAD},\n], weight_decay=WD)\n\n# Hybrid loss\nclass HybridLoss(nn.Module):\n    def __init__(self, alpha_per_class, gamma=1.5):\n        super().__init__()\n        self.alpha = alpha_per_class\n        self.gamma = gamma\n    def forward(self, logits, target):\n        # Use focal if target class has alpha>1, else CE\n        alphas = torch.ones(logits.size(0), device=logits.device)\n        use_focal = torch.zeros_like(alphas, dtype=torch.bool)\n        for i, t in enumerate(target.tolist()):\n            if t in self.alpha:\n                alphas[i] = self.alpha[t]\n                use_focal[i] = True\n        ce = F.cross_entropy(logits, target, reduction=\"none\")\n        if use_focal.any():\n            pt = torch.softmax(logits, dim=1).gather(1, target.view(-1,1)).squeeze(1).clamp(1e-6, 1-1e-6)\n            fl = ((1-pt)**self.gamma) * ce * alphas\n            # mix: use focal where flagged, CE elsewhere\n            mix = torch.where(use_focal, fl, ce)\n            return mix.mean()\n        else:\n            return ce.mean()\n\ncriterion = HybridLoss(alpha_per_class=alpha_per_class, gamma=1.5)\n\n# ---- Train/Eval helpers ----\n@torch.no_grad()\ndef eval_top1(loader):\n    model.eval()\n    correct = 0; total = 0\n    for x,y in loader:\n        x = x.to(DEVICE); y = y.to(DEVICE)\n        logits = model(x)\n        pred = logits.argmax(1)\n        correct += (pred==y).sum().item()\n        total += y.numel()\n    return correct/total if total>0 else 0.0\n\nbest_acc = eval_top1(val_loader)\nprint(\"Start val top1:\", best_acc)\n\n# ---- Train loop ----\nfor epoch in range(EPOCHS):\n    model.train()\n    for x,y in train_loader:\n        x = x.to(DEVICE); y = y.to(DEVICE)\n        logits = model(x)\n        loss = criterion(logits, y)\n        opt.zero_grad(set_to_none=True)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        opt.step()\n    acc = eval_top1(val_loader)\n    print(f\"Epoch {epoch+1}/{EPOCHS} val top1: {acc:.4f}\")\n    if acc >= best_acc - 1e-4:\n        best_acc = acc\n        torch.save({\"model\": model.state_dict(), \"label2idx\": label2idx}, CKPT_OUT)\n        print(\"Saved:\", CKPT_OUT)\n\nprint(\"Best val top1:\", best_acc)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# one_shot_eval_v6.py\nimport os, json, cv2, torch, timm, numpy as np, pandas as pd\nfrom pathlib import Path\nfrom torch import nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\n# ---- Config ----\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nIM_SIZE = 320\nIM_MEAN = [0.485, 0.456, 0.406]\nIM_STD  = [0.229, 0.224, 0.225]\nTEMP = 0.551  # deployment temperature\n\n# Paths (edit these three as needed)\nCKPT_PATH = \"/kaggle/working/v6_global_eval/effb3_v6_unified_ft.pt\"  # new fine-tuned model\nMAP_JSON  = \"/kaggle/input/k/adiithape/cnn-model-v3/outputs/metadata/label2idx_v6.json\"\nTEST_ROOT = \"/kaggle/input/plant-disease-google-test-images/test_google/test\"\n\n# Metadata\nVAL_CSV  = \"/kaggle/input/k/adiithape/cnn-model-v3/outputs/metadata/curated_val_v6.csv\"\n\n# Output dir\nOUT = Path(\"/kaggle/working/v6_global_eval/new_eval\"); OUT.mkdir(parents=True, exist_ok=True)\n\n# ---- I/O helpers ----\ndef list_images(root):\n    exts = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\",\".tif\",\".tiff\"}\n    return [str(p) for p in Path(root).rglob(\"*\") if p.suffix.lower() in exts]\n\ndef load_rgb(path):\n    img = cv2.imread(str(path))\n    if img is None: \n        raise FileNotFoundError(path)\n    return img  # BGR\n\ndef preprocess_bgr_pad(img_bgr):\n    h,w = img_bgr.shape[:2]; s = IM_SIZE/max(h,w)\n    nh,nw = int(h*s), int(w*s)\n    img = cv2.resize(img_bgr,(nw,nh), interpolation=cv2.INTER_AREA)\n    canvas = np.zeros((IM_SIZE,IM_SIZE,3), dtype=img.dtype)\n    y0=(IM_SIZE-nh)//2; x0=(IM_SIZE-nw)//2\n    canvas[y0:y0+nh, x0:x0+nw] = img\n    x = canvas[:,:,::-1].astype(np.float32)/255.0\n    x = (x - np.array(IM_MEAN,np.float32))/np.array(IM_STD,np.float32)\n    return torch.from_numpy(np.transpose(x,(2,0,1))).float()\n\ndef preprocess_bgr_center_crop(img_bgr):\n    h,w = img_bgr.shape[:2]\n    if h < w: nh,nw = IM_SIZE, int(w*IM_SIZE/h)\n    else:     nh,nw = int(h*IM_SIZE/w), IM_SIZE\n    img = cv2.resize(img_bgr,(nw,nh), interpolation=cv2.INTER_AREA)\n    y0=(nh-IM_SIZE)//2; x0=(nw-IM_SIZE)//2\n    img = img[y0:y0+IM_SIZE, x0:x0+IM_SIZE]\n    x = img[:,:,::-1].astype(np.float32)/255.0\n    x = (x - np.array(IM_MEAN,np.float32))/np.array(IM_STD,np.float32)\n    return torch.from_numpy(np.transpose(x,(2,0,1))).float()\n\n# ---- Load label map and model ----\nwith open(MAP_JSON, \"r\") as f:\n    label2idx = json.load(f)\nidx2label = {int(v):k for k,v in label2idx.items()}\nclasses = [idx2label[i] for i in range(len(idx2label))]\n\nmodel = timm.create_model(\"efficientnet_b3\", pretrained=False, num_classes=len(classes)).to(DEVICE).eval()\nstate = torch.load(CKPT_PATH, map_location=\"cpu\")\nsd = state[\"model\"] if isinstance(state, dict) and \"model\" in state and isinstance(state[\"model\"], dict) else state\nmodel.load_state_dict(sd, strict=True)\n\n# ---- Predictors ----\n@torch.no_grad()\ndef predict_dualT_logits(img_bgr, temperature=TEMP):\n    x1 = preprocess_bgr_pad(img_bgr).unsqueeze(0).to(DEVICE)\n    x2 = preprocess_bgr_center_crop(img_bgr).unsqueeze(0).to(DEVICE)\n    z = (model(x1) + model(x2)) / 2.0\n    if temperature is not None and temperature > 0:\n        z = z / float(temperature)\n    return z.squeeze(0).float().cpu()\n\n@torch.no_grad()\ndef predict_file(path, topk=3):\n    img = load_rgb(path)\n    z = predict_dualT_logits(img, TEMP)\n    p = F.softmax(z, dim=0).numpy()\n    top = p.argsort()[::-1][:topk]\n    out = [(classes[i], float(p[i])) for i in top]\n    return out, p\n\n# ---- Build val/test DataFrames (all species) ----\nval_df = pd.read_csv(VAL_CSV)[[\"filepath\",\"label\"]].reset_index(drop=True)\n# Test_root enumeration\nrows = []\nfor p in list_images(TEST_ROOT):\n    lbl = Path(p).parent.name\n    if lbl in label2idx:\n        rows.append({\"filepath\": p, \"label\": lbl})\ntest_df = pd.DataFrame(rows).reset_index(drop=True)\n\n# ---- Inference cache ----\ndef run_infer_cache(df, out_csv):\n    rows = []\n    for i, r in df.iterrows():\n        fp, gt = r[\"filepath\"], r[\"label\"]\n        try:\n            preds, p = predict_file(fp, topk=3)\n        except Exception:\n            continue\n        top = np.argsort(-p)[:3]\n        row = {\n            \"filepath\": fp, \"gt\": gt,\n            \"pred\": classes[top[0]], \"prob\": float(p[top[0]]),\n            \"pred1\": classes[top[0]], \"prob1\": float(p[top[0]]),\n            \"pred2\": classes[top[1]] if len(top)>1 else \"\", \"prob2\": float(p[top[1]]) if len(top)>1 else 0.0,\n            \"pred3\": classes[top[2]] if len(top)>2 else \"\", \"prob3\": float(p[top[2]]) if len(top)>2 else 0.0,\n        }\n        rows.append(row)\n        if (i+1) % 500 == 0:\n            print(f\"{i+1}/{len(df)}\")\n    pd.DataFrame(rows).to_csv(out_csv, index=False)\n    print(\"Saved:\", out_csv)\n\nVAL_PRED = OUT/\"preds_v6_val_dualT.csv\"\nTEST_PRED = OUT/\"preds_v6_test_dualT.csv\"\nrun_infer_cache(val_df, VAL_PRED)\nrun_infer_cache(test_df, TEST_PRED)\n\n# ---- Metrics & confusions ----\nval = pd.read_csv(VAL_PRED)\nlabels = sorted(pd.concat([val[\"gt\"], val[\"pred\"]]).unique())\nlab2idx_local = {l:i for i,l in enumerate(labels)}\nidx2lab_local = {i:l for l,i in lab2idx_local.items()}\n\ndef per_class_report(df):\n    y_true = df[\"gt\"].map(lab2idx_local).to_numpy()\n    y_pred = df[\"pred\"].map(lab2idx_local).to_numpy()\n    n = len(labels)\n    cm = np.zeros((n,n), dtype=np.int64)\n    for t,pred in zip(y_true, y_pred):\n        cm[t,pred] += 1\n    support = cm.sum(1)\n    tp = np.diag(cm); fp = cm.sum(0) - tp; fn = cm.sum(1) - tp\n    precision = np.divide(tp, tp+fp, out=np.zeros_like(tp, dtype=float), where=(tp+fp)>0)\n    recall    = np.divide(tp, tp+fn, out=np.zeros_like(tp, dtype=float), where=(tp+fn)>0)\n    f1 = np.divide(2*precision*recall, precision+recall, out=np.zeros_like(tp, dtype=float), where=(precision+recall)>0)\n    rep = pd.DataFrame({\n        \"label\":[idx2lab_local[i] for i in range(n)],\n        \"support\":support, \"precision\":precision, \"recall\":recall, \"f1\":f1\n    }).sort_values(\"f1\")\n    macro = {\n        \"macro_precision\": float(np.mean(precision)),\n        \"macro_recall\": float(np.mean(recall)),\n        \"macro_f1\": float(np.mean(f1)),\n        \"overall_acc\": float(tp.sum()/cm.sum()) if cm.sum()>0 else 0.0\n    }\n    return cm, rep, macro\n\ncm, rep, macro = per_class_report(val)\npd.DataFrame(cm, index=labels, columns=labels).to_csv(OUT/\"confusion_matrix.csv\", index=False)\nrep.to_csv(OUT/\"per_class_report.csv\", index=False)\nwith open(OUT/\"summary.json\",\"w\") as f: json.dump(macro, f, indent=2)\n\n# Top confusions\npairs = []\nfor i in range(cm.shape[0]):\n    row = cm[i].copy(); row[i] = 0\n    if row.sum()==0: continue\n    top_js = np.argsort(-row)[:min(50, cm.shape[1]-1)]\n    for j in top_js:\n        if row[j] > 0:\n            pairs.append({\"true\": labels[i], \"pred\": labels[j], \"count\": int(row[j])})\npairs.sort(key=lambda d: d[\"count\"], reverse=True)\nwith open(OUT/\"top_confusions.json\",\"w\") as f:\n    json.dump({\"global\": pairs[:100]}, f, indent=2)\n\nprint(\"Saved metrics and confusions to:\", str(OUT))\n\n# ---- CAM overlap (all species) ----\n# Target conv layer: conv_head preferred\ntarget_layer = getattr(model, \"conv_head\", None)\nif target_layer is None:\n    # fallback last Conv2d\n    tname = None\n    for name, module in model.named_modules():\n        if isinstance(module, nn.Conv2d):\n            tname = name\n    def get_module_by_name(model, name):\n        mod = model\n        for part in name.split(\".\"):\n            mod = getattr(mod, part)\n        return mod\n    target_layer = get_module_by_name(model, tname)\nprint(\"CAM target:\", target_layer.__class__.__name__)\n\ndef gradcam_on_image(img_bgr, cls_idx, target_layer):\n    model.zero_grad(set_to_none=True)\n    activations, gradients = [], []\n    def fwd_hook(m, i, o): activations.append(o.detach())\n    def bwd_hook(m, gi, go): gradients.append(go[0].detach())\n    h1 = target_layer.register_forward_hook(fwd_hook)\n    h2 = target_layer.register_full_backward_hook(bwd_hook)\n\n    x = preprocess_bgr_center_crop(img_bgr).unsqueeze(0).to(DEVICE)\n    out = model(x); score = out[0, cls_idx]; score.backward()\n    act = activations[-1][0]; grad = gradients[-1][0]\n    w = grad.mean(dim=(1,2), keepdim=True)\n    cam = (w * act).sum(0).cpu().numpy()\n    cam = np.maximum(cam, 0); cam = cv2.resize(cam, (IM_SIZE, IM_SIZE))\n    cam = (cam - cam.min()) / (cam.max() + 1e-6)\n    h1.remove(); h2.remove()\n    return cam\n\ndef lesion_mask(img_bgr):\n    imgL = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2LAB)[:,:,0]\n    imgL = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8)).apply(imgL)\n    edges = cv2.Canny(imgL, 60, 120)\n    edges = cv2.dilate(edges, np.ones((3,3),np.uint8), iterations=1)\n    edges = cv2.GaussianBlur(edges, (3,3), 0)\n    mask = (edges > 0).astype(np.uint8)\n    mask = cv2.resize(mask, (IM_SIZE, IM_SIZE), interpolation=cv2.INTER_NEAREST)\n    return mask\n\npreds = val.copy()\npreds[\"correct\"] = preds[\"gt\"] == preds[\"pred\"]\ncls2idx = {c:i for i,c in enumerate(classes)}\n\nPANEL_DIR = OUT/\"cam_panels\"; PANEL_DIR.mkdir(exist_ok=True)\nmetrics = []\nfor lbl in classes:\n    if lbl not in preds[\"gt\"].values or lbl not in cls2idx: \n        continue\n    sub = preds[preds[\"gt\"]==lbl].sample(min(16, (preds[\"gt\"]==lbl).sum()), random_state=42)\n    figs = []\n    for _, r in sub.iterrows():\n        fp = r[\"filepath\"]\n        try:\n            img = load_rgb(fp)\n        except Exception:\n            continue\n        cam = gradcam_on_image(img, cls2idx[lbl], target_layer)\n        msk = lesion_mask(img)\n        overlap = float((cam * (msk>0)).sum() / (cam.sum() + 1e-6))\n        metrics.append({\"label\": lbl, \"filepath\": fp, \"overlap\": overlap, \"pred\": r[\"pred\"], \"correct\": bool(r[\"correct\"])})\n        heat = (cv2.applyColorMap((cam*255).astype(np.uint8), cv2.COLORMAP_JET)[:,:,::-1])/255.0\n        rgb  = cv2.cvtColor(cv2.resize(img, (IM_SIZE,IM_SIZE)), cv2.COLOR_BGR2RGB)/255.0\n        overlay = 0.4*heat + 0.6*rgb\n        figs.append(overlay)\n    if figs:\n        cols = 4; rows = int(np.ceil(len(figs)/cols))\n        plt.figure(figsize=(3*cols, 3*rows))\n        for i, im in enumerate(figs, 1):\n            plt.subplot(rows, cols, i); plt.imshow(im); plt.axis(\"off\")\n        plt.suptitle(f\"Grad-CAM — {lbl}\"); plt.tight_layout()\n        plt.savefig(PANEL_DIR/f\"cam_{lbl}.png\", dpi=200); plt.close()\n\npd.DataFrame(metrics).to_csv(OUT/\"cam_overlap_all_species.csv\", index=False)\nprint(\"Saved CAM overlap and panels to:\", str(OUT))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Short correction pass","metadata":{}},{"cell_type":"code","source":"# 3_finetune_one_epoch.py\nimport os, json, cv2, torch, timm, numpy as np, pandas as pd\nfrom pathlib import Path\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# Config\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nIM_SIZE = 320\nIM_MEAN = [0.485, 0.456, 0.406]; IM_STD = [0.229, 0.224, 0.225]\nEPOCHS = 1\nBS_TRAIN = 32; BS_VAL = 64\nLR_BACKBONE = 1e-5; LR_HEAD = 3e-5; WD = 1e-2\n\n# Paths\nTRAIN_CSV = \"/kaggle/input/k/adiithape/cnn-model-v3/outputs/metadata/curated_train_v6.csv\"\nVAL_CSV   = \"/kaggle/input/k/adiithape/cnn-model-v3/outputs/metadata/curated_val_v6.csv\"\nMAP_JSON  = \"/kaggle/input/k/adiithape/cnn-model-v3/outputs/metadata/label2idx_v6.json\"\nCKPT_IN   = \"/kaggle/working/v6_global_eval/effb3_v6_unified_ft.pt\"  # previous best\nCKPT_OUT  = \"/kaggle/working/v6_global_eval/effb3_v6_unified_ft_final.pt\"\nCURR      = Path(\"/kaggle/working/v6_global_eval/curriculum\")\n\n# Load metadata and curriculum\ntrain_df = pd.read_csv(TRAIN_CSV)\nval_df   = pd.read_csv(VAL_CSV)\nwith open(MAP_JSON,\"r\") as f: label2idx = json.load(f)\nidx2label = {int(v):k for k,v in label2idx.items()}\nclasses = [idx2label[i] for i in range(len(idx2label))]\n\nwith open(CURR/\"aug_map.json\",\"r\") as f: aug_map = json.load(f)\nwith open(CURR/\"w_label.json\",\"r\") as f: w_label = json.load(f)\nalpha_per_class = {}\nAPC = CURR/\"alpha_per_class.json\"\nif APC.exists():\n    with open(APC,\"r\") as f: alpha_per_class = {int(k):float(v) for k,v in json.load(f).items()}\n\ndef sharpen_unsharp(img):\n    blur = cv2.GaussianBlur(img, (0,0), 1.0)\n    return cv2.addWeighted(img, 1.5, blur, -0.5, 0)\ndef sharpen_unsharp_aug(img, **kwargs): return sharpen_unsharp(img)\n\ndef make_transform_for_label(lbl: str):\n    cfg = aug_map.get(lbl, {\n        \"center_crop_p\": 0.10, \"blur_p\": 0.10, \"sharpen_p\": 0.00,\n        \"color_jitter_p\": 0.08, \"cj_brightness\":0.10, \"cj_contrast\":0.10, \"cj_saturation\":0.10, \"cj_hue\":0.02,\n    })\n    for k, v in {\"center_crop_p\":0.10,\"blur_p\":0.10,\"sharpen_p\":0.00,\"color_jitter_p\":0.08,\n                 \"cj_brightness\":0.10,\"cj_contrast\":0.10,\"cj_saturation\":0.10,\"cj_hue\":0.02}.items():\n        cfg.setdefault(k, v)\n    tfs = []\n    if cfg[\"center_crop_p\"] > 0:\n        tfs.append(A.RandomResizedCrop(size=(IM_SIZE,IM_SIZE), scale=(0.85,1.0), ratio=(0.9,1.1),\n                                       interpolation=cv2.INTER_AREA, p=cfg[\"center_crop_p\"]))\n    else:\n        tfs.append(A.Resize(IM_SIZE, IM_SIZE, interpolation=cv2.INTER_AREA))\n    tfs.append(A.HorizontalFlip(p=0.5))\n    tfs.append(A.Affine(scale=(0.95,1.05), translate_percent={\"x\":(-0.02,0.02),\"y\":(-0.02,0.02)},\n                        rotate=(-15,15), mode=cv2.BORDER_REFLECT_101, p=0.35))\n    if cfg[\"color_jitter_p\"] > 0:\n        tfs.append(A.ColorJitter(brightness=cfg[\"cj_brightness\"], contrast=cfg[\"cj_contrast\"],\n                                 saturation=cfg[\"cj_saturation\"], hue=cfg[\"cj_hue\"], p=cfg[\"color_jitter_p\"]))\n    if cfg[\"blur_p\"] > 0: tfs.append(A.GaussianBlur(blur_limit=3, p=cfg[\"blur_p\"]))\n    if cfg[\"sharpen_p\"] > 0: tfs.append(A.Lambda(image=sharpen_unsharp_aug, p=cfg[\"sharpen_p\"]))\n    tfs.append(A.ImageCompression(quality=(70,95), p=0.15))\n    tfs.append(A.Normalize(mean=IM_MEAN, std=IM_STD)); tfs.append(ToTensorV2())\n    return A.Compose(tfs)\n\nclass PlantDS(Dataset):\n    def __init__(self, df): self.df = df.reset_index(drop=True)\n    def __len__(self): return len(self.df)\n    def __getitem__(self, i):\n        r = self.df.iloc[i]\n        img = cv2.imread(r[\"filepath\"], cv2.IMREAD_COLOR)\n        if img is None: img = np.zeros((IM_SIZE,IM_SIZE,3), np.uint8)\n        x = make_transform_for_label(r[\"label\"])(image=img)[\"image\"]\n        y = label2idx[r[\"label\"]]\n        return x, y\n\ndef build_weights(df):\n    w = np.ones(len(df), dtype=np.float32)\n    for i, (_, row) in enumerate(df.iterrows()):\n        lbl = row[\"label\"]\n        w[i] *= float(w_label.get(lbl, 1.0))\n    return w\n\ntrain_loader = DataLoader(PlantDS(train_df), batch_size=BS_TRAIN,\n                          sampler=WeightedRandomSampler(build_weights(train_df), num_samples=len(train_df), replacement=True),\n                          num_workers=2, pin_memory=True)\nval_loader = DataLoader(PlantDS(val_df), batch_size=BS_VAL, shuffle=False, num_workers=2, pin_memory=True)\n\n# Model\nmodel = timm.create_model(\"efficientnet_b3\", pretrained=False, num_classes=len(classes)).to(DEVICE)\nstate = torch.load(CKPT_IN, map_location=\"cpu\")\nsd = state[\"model\"] if isinstance(state, dict) and \"model\" in state and isinstance(state[\"model\"], dict) else state\nmodel.load_state_dict(sd, strict=True)\n\n# Optim\nbackbone_params, head_params = [], []\nfor n,p in model.named_parameters():\n    if not p.requires_grad: continue\n    (head_params if \"classifier\" in n else backbone_params).append(p)\nopt = torch.optim.AdamW([\n    {\"params\": backbone_params, \"lr\": LR_BACKBONE},\n    {\"params\": head_params, \"lr\": LR_HEAD},\n], weight_decay=WD)\n\nclass HybridLoss(nn.Module):\n    def __init__(self, alpha_per_class, gamma=1.5):\n        super().__init__(); self.alpha = alpha_per_class; self.gamma = gamma\n    def forward(self, logits, target):\n        ce = F.cross_entropy(logits, target, reduction=\"none\")\n        alphas = torch.ones_like(target, dtype=torch.float, device=logits.device)\n        use = torch.zeros_like(target, dtype=torch.bool, device=logits.device)\n        for i, t in enumerate(target.tolist()):\n            if t in self.alpha: alphas[i] = self.alpha[t]; use[i] = True\n        if use.any():\n            pt = torch.softmax(logits, dim=1).gather(1, target.view(-1,1)).squeeze(1).clamp(1e-6, 1-1e-6)\n            fl = ((1-pt)**self.gamma) * ce * alphas\n            return torch.where(use, fl, ce).mean()\n        return ce.mean()\n\ncriterion = HybridLoss(alpha_per_class=alpha_per_class, gamma=1.5)\n\n@torch.no_grad()\ndef eval_top1(loader):\n    model.eval(); correct=0; total=0\n    for x,y in loader:\n        x=x.to(DEVICE); y=y.to(DEVICE)\n        pred = model(x).argmax(1)\n        correct += (pred==y).sum().item(); total += y.numel()\n    return correct/total if total>0 else 0.0\n\nbest_acc = eval_top1(val_loader)\nprint(\"Start val top1:\", best_acc)\n\n# Train one epoch\nmodel.train()\nfor x,y in train_loader:\n    x=x.to(DEVICE); y=y.to(DEVICE)\n    logits = model(x); loss = criterion(logits, y)\n    opt.zero_grad(set_to_none=True); loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n    opt.step()\n\nacc = eval_top1(val_loader)\nprint(\"Post-epoch val top1:\", acc)\nif acc >= best_acc - 1e-4:\n    torch.save({\"model\": model.state_dict(), \"label2idx\": label2idx}, CKPT_OUT)\n    print(\"Saved:\", CKPT_OUT)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Final evaluation","metadata":{}},{"cell_type":"code","source":"# 4_eval_final_checkpoint.py\n# Set CKPT_PATH to the new final model and rerun the one-shot eval you used earlier.\nimport os, json, cv2, torch, timm, numpy as np, pandas as pd\nfrom pathlib import Path\nfrom torch import nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nIM_SIZE = 320; IM_MEAN=[0.485,0.456,0.406]; IM_STD=[0.229,0.224,0.225]; TEMP=0.551\n\nCKPT_PATH = \"/kaggle/working/v6_global_eval/effb3_v6_unified_ft_final.pt\"\nMAP_JSON  = \"/kaggle/input/k/adiithape/cnn-model-v3/outputs/metadata/label2idx_v6.json\"\nVAL_CSV   = \"/kaggle/input/k/adiithape/cnn-model-v3/outputs/metadata/curated_val_v6.csv\"\nTEST_ROOT = \"/kaggle/input/plant-disease-google-test-images/test_google/test\"\nOUT = Path(\"/kaggle/working/v6_global_eval/final_eval\"); OUT.mkdir(parents=True, exist_ok=True)\n\ndef list_images(root):\n    exts={\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\",\".tif\",\".tiff\"}\n    return [str(p) for p in Path(root).rglob(\"*\") if p.suffix.lower() in exts]\ndef load_rgb(path):\n    img=cv2.imread(str(path)); \n    if img is None: raise FileNotFoundError(path)\n    return img\ndef preprocess_bgr_pad(img_bgr):\n    h,w=img_bgr.shape[:2]; s=IM_SIZE/max(h,w)\n    nh,nw=int(h*s),int(w*s); img=cv2.resize(img_bgr,(nw,nh),interpolation=cv2.INTER_AREA)\n    canvas=np.zeros((IM_SIZE,IM_SIZE,3),dtype=img.dtype); y0=(IM_SIZE-nh)//2; x0=(IM_SIZE-nw)//2\n    canvas[y0:y0+nh, x0:x0+nw]=img\n    x=canvas[:,:,::-1].astype(np.float32)/255.0\n    x=(x-np.array(IM_MEAN,np.float32))/np.array(IM_STD,np.float32)\n    return torch.from_numpy(np.transpose(x,(2,0,1))).float()\ndef preprocess_bgr_center_crop(img_bgr):\n    h,w=img_bgr.shape[:2]\n    if h<w: nh,nw=IM_SIZE,int(w*IM_SIZE/h)\n    else:   nh,nw=int(h*IM_SIZE/w),IM_SIZE\n    img=cv2.resize(img_bgr,(nw,nh),interpolation=cv2.INTER_AREA)\n    y0=(nh-IM_SIZE)//2; x0=(nw-IM_SIZE)//2; img=img[y0:y0+IM_SIZE, x0:x0+IM_SIZE]\n    x=img[:,:,::-1].astype(np.float32)/255.0\n    x=(x-np.array(IM_MEAN,np.float32))/np.array(IM_STD,np.float32)\n    return torch.from_numpy(np.transpose(x,(2,0,1))).float()\n\nwith open(MAP_JSON,\"r\") as f: label2idx=json.load(f)\nidx2label={int(v):k for k,v in label2idx.items()}\nclasses=[idx2label[i] for i in range(len(idx2label))]\n\nmodel=timm.create_model(\"efficientnet_b3\", pretrained=False, num_classes=len(classes)).to(DEVICE).eval()\nstate=torch.load(CKPT_PATH, map_location=\"cpu\")\nsd=state[\"model\"] if isinstance(state, dict) and \"model\" in state and isinstance(state[\"model\"], dict) else state\nmodel.load_state_dict(sd, strict=True)\n\n@torch.no_grad()\ndef predict_dualT_logits(img_bgr, temperature=TEMP):\n    x1=preprocess_bgr_pad(img_bgr).unsqueeze(0).to(DEVICE)\n    x2=preprocess_bgr_center_crop(img_bgr).unsqueeze(0).to(DEVICE)\n    z=(model(x1)+model(x2))/2.0\n    if temperature and temperature>0: z=z/float(temperature)\n    return z.squeeze(0).float().cpu()\n\ndef run_infer_cache(df, out_csv):\n    rows=[]\n    for i,r in df.iterrows():\n        fp, gt=r[\"filepath\"], r[\"label\"]\n        try:\n            img=load_rgb(fp); z=predict_dualT_logits(img, TEMP); p=F.softmax(z, dim=0).numpy()\n        except Exception:\n            continue\n        top=np.argsort(-p)[:3]\n        rows.append({\n            \"filepath\":fp,\"gt\":gt,\n            \"pred\":classes[top[0]],\"prob\":float(p[top[0]]),\n            \"pred1\":classes[top[0]],\"prob1\":float(p[top[0]]),\n            \"pred2\":classes[top[1]] if len(top)>1 else \"\",\"prob2\":float(p[top[1]]) if len(top)>1 else 0.0,\n            \"pred3\":classes[top[2]] if len(top)>2 else \"\",\"prob3\":float(p[top[2]]) if len(top)>2 else 0.0,\n        })\n        if (i+1)%500==0: print(f\"{i+1}/{len(df)}\")\n    pd.DataFrame(rows).to_csv(out_csv, index=False); print(\"Saved:\", out_csv)\n\nval_df=pd.read_csv(VAL_CSV)[[\"filepath\",\"label\"]].reset_index(drop=True)\nrows=[]; \nfor p in list_images(TEST_ROOT):\n    lbl=Path(p).parent.name\n    if lbl in label2idx: rows.append({\"filepath\":p,\"label\":lbl})\ntest_df=pd.DataFrame(rows).reset_index(drop=True)\n\nVAL_PRED=OUT/\"preds_v6_val_dualT.csv\"\nTEST_PRED=OUT/\"preds_v6_test_dualT.csv\"\nrun_infer_cache(val_df, VAL_PRED)\nrun_infer_cache(test_df, TEST_PRED)\n\n# Metrics/confusions\nval=pd.read_csv(VAL_PRED)\nlabels=sorted(pd.concat([val[\"gt\"], val[\"pred\"]]).unique())\nlab2idx_local={l:i for i,l in enumerate(labels)}; idx2lab_local={i:l for l,i in lab2idx_local.items()}\nn=len(labels); cm=np.zeros((n,n), dtype=np.int64)\nfor t,pred in zip(val[\"gt\"].map(lab2idx_local).to_numpy(), val[\"pred\"].map(lab2idx_local).to_numpy()):\n    cm[t,pred]+=1\nsupport=cm.sum(1); tp=np.diag(cm); fp=cm.sum(0)-tp; fn=cm.sum(1)-tp\nprecision=np.divide(tp, tp+fp, out=np.zeros_like(tp,dtype=float), where=(tp+fp)>0)\nrecall   =np.divide(tp, tp+fn, out=np.zeros_like(tp,dtype=float), where=(tp+fn)>0)\nf1=np.divide(2*precision*recall, precision+recall, out=np.zeros_like(tp,dtype=float), where=(precision+recall)>0)\nrep=pd.DataFrame({\"label\":[idx2lab_local[i] for i in range(n)],\"support\":support,\"precision\":precision,\"recall\":recall,\"f1\":f1}).sort_values(\"f1\")\nmacro={\"macro_precision\":float(np.mean(precision)),\"macro_recall\":float(np.mean(recall)),\"macro_f1\":float(np.mean(f1)),\"overall_acc\":float(tp.sum()/cm.sum()) if cm.sum()>0 else 0.0}\npd.DataFrame(cm, index=labels, columns=labels).to_csv(OUT/\"confusion_matrix.csv\", index=False)\nrep.to_csv(OUT/\"per_class_report.csv\", index=False)\nwith open(OUT/\"summary.json\",\"w\") as f: json.dump(macro, f, indent=2)\n\npairs=[]\nfor i in range(cm.shape[0]):\n    row=cm[i].copy(); row[i]=0\n    if row.sum()==0: continue\n    for j in np.argsort(-row)[:min(50, cm.shape[1]-1)]:\n        if row[j]>0: pairs.append({\"true\":labels[i],\"pred\":labels[j],\"count\":int(row[j])})\npairs.sort(key=lambda d: d[\"count\"], reverse=True)\npd.DataFrame(pairs[:50]).to_csv(OUT/\"top_confusions_new.csv\", index=False)\nprint(\"Saved metrics/confusions to:\", str(OUT))\n\n# CAM overlap (same as earlier)\ntarget_layer=getattr(model,\"conv_head\",None)\nif target_layer is None:\n    tname=None\n    for name, module in model.named_modules():\n        if isinstance(module, nn.Conv2d): tname=name\n    def get_module_by_name(model, name):\n        mod=model\n        for part in name.split(\".\"): mod=getattr(mod, part)\n        return mod\n    target_layer=get_module_by_name(model, tname)\nprint(\"CAM target:\", target_layer.__class__.__name__)\n\ndef gradcam_on_image(img_bgr, cls_idx, target_layer):\n    model.zero_grad(set_to_none=True); activations=[]; gradients=[]\n    def fwd_hook(m,i,o): activations.append(o.detach())\n    def bwd_hook(m,gi,go): gradients.append(go[0].detach())\n    h1=target_layer.register_forward_hook(fwd_hook)\n    h2=target_layer.register_full_backward_hook(bwd_hook)\n    x=preprocess_bgr_center_crop(img_bgr).unsqueeze(0).to(DEVICE)\n    out=model(x); score=out[0, cls_idx]; score.backward()\n    act=activations[-1][0]; grad=gradients[-1][0]; w=grad.mean(dim=(1,2), keepdim=True)\n    cam=(w*act).sum(0).cpu().numpy(); cam=np.maximum(cam,0); cam=cv2.resize(cam,(IM_SIZE,IM_SIZE))\n    cam=(cam-cam.min())/(cam.max()+1e-6); h1.remove(); h2.remove(); return cam\n\ndef lesion_mask(img_bgr):\n    imgL=cv2.cvtColor(img_bgr, cv2.COLOR_BGR2LAB)[:,:,0]\n    imgL=cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8)).apply(imgL)\n    edges=cv2.Canny(imgL,60,120); edges=cv2.dilate(edges, np.ones((3,3),np.uint8),1); edges=cv2.GaussianBlur(edges,(3,3),0)\n    mask=(edges>0).astype(np.uint8); mask=cv2.resize(mask,(IM_SIZE,IM_SIZE),interpolation=cv2.INTER_NEAREST); return mask\n\npreds=val.copy(); preds[\"correct\"]=preds[\"gt\"]==preds[\"pred\"]; cls2idx={c:i for i,c in enumerate(classes)}\nPANEL=OUT/\"cam_panels\"; PANEL.mkdir(exist_ok=True)\nrows=[]\nfor lbl in classes:\n    if lbl not in preds[\"gt\"].values or lbl not in cls2idx: continue\n    sub=preds[preds[\"gt\"]==lbl].sample(min(16, (preds[\"gt\"]==lbl).sum()), random_state=42)\n    figs=[]\n    for _, r in sub.iterrows():\n        fp=r[\"filepath\"]\n        try: img=load_rgb(fp)\n        except Exception: continue\n        cam=gradcam_on_image(img, cls2idx[lbl], target_layer); msk=lesion_mask(img)\n        overlap=float((cam*(msk>0)).sum()/(cam.sum()+1e-6))\n        rows.append({\"label\":lbl,\"filepath\":fp,\"overlap\":overlap,\"pred\":r[\"pred\"],\"correct\":bool(r[\"correct\"])})\n        heat=(cv2.applyColorMap((cam*255).astype(np.uint8), cv2.COLORMAP_JET)[:,:,::-1])/255.0\n        rgb=cv2.cvtColor(cv2.resize(img,(IM_SIZE,IM_SIZE)), cv2.COLOR_BGR2RGB)/255.0\n        figs.append(0.4*heat+0.6*rgb)\n    if figs:\n        cols=4; rows_n=int(np.ceil(len(figs)/cols)); plt.figure(figsize=(3*cols,3*rows_n))\n        for i, im in enumerate(figs,1):\n            plt.subplot(rows_n, cols, i); plt.imshow(im); plt.axis(\"off\")\n        plt.suptitle(f\"Grad-CAM — {lbl}\"); plt.tight_layout(); plt.savefig(PANEL/f\"cam_{lbl}.png\", dpi=200); plt.close()\n\npd.DataFrame(rows).to_csv(OUT/\"cam_overlap_all_species.csv\", index=False)\nprint(\"Saved CAM overlap/panels to:\", str(OUT))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Compare vs previous","metadata":{}},{"cell_type":"code","source":"# compare_final_vs_prev.py\nimport json, pandas as pd, numpy as np\nfrom pathlib import Path\n\n# Previous iteration artifacts (from earlier run)\nPREV = Path(\"/kaggle/working/v6_global_eval/new_eval\")\nprev_sum = json.load(open(PREV/\"summary.json\"))\nprev_rep = pd.read_csv(PREV/\"per_class_report.csv\")\nprev_conf = pd.read_csv(PREV/\"top_confusions_new.csv\") if (PREV/\"top_confusions_new.csv\").exists() else None\nprev_cam = pd.read_csv(PREV/\"cam_overlap_all_species.csv\" if (PREV/\"cam_overlap_all_species.csv\").exists() else PREV/\"cam_overlap_all_species-1.csv\")\n\n# Current final artifacts\nCURR = Path(\"/kaggle/working/v6_global_eval/final_eval\")\ncurr_sum = json.load(open(CURR/\"summary.json\"))\ncurr_rep = pd.read_csv(CURR/\"per_class_report.csv\")\ncurr_conf = pd.read_csv(CURR/\"top_confusions_new.csv\")\ncurr_cam = pd.read_csv(CURR/\"cam_overlap_all_species.csv\")\n\nprint(\"Prev macro:\", prev_sum)\nprint(\"Curr macro:\", curr_sum)\n\n# Per-class F1 deltas\nrep_join = prev_rep.merge(curr_rep, on=\"label\", suffixes=(\"_prev\",\"_curr\"))\nrep_join[\"d_f1\"] = rep_join[\"f1_curr\"] - rep_join[\"f1_prev\"]\nprint(\"\\nBottom 10 classes (prev) with F1 delta:\")\nprev_bottom = prev_rep.sort_values(\"f1\").head(10)[\"label\"].tolist()\nprint(rep_join[rep_join[\"label\"].isin(prev_bottom)][[\"label\",\"f1_prev\",\"f1_curr\",\"d_f1\"]])\n\n# Confusion deltas for overlapping pairs (if prev available)\nif prev_conf is not None:\n    # Normalize to tuple keys\n    pc = prev_conf.copy(); cc = curr_conf.copy()\n    # Some files might be JSON-rows vs CSV; ensure columns exist\n    for df in (pc, cc):\n        if \"true\" not in df.columns or \"pred\" not in df.columns or \"count\" not in df.columns:\n            raise RuntimeError(\"Confusion CSV must have true, pred, count columns\")\n    key = [\"true\",\"pred\"]\n    conf_merge = pc.merge(cc, on=key, how=\"outer\", suffixes=(\"_prev\",\"_curr\")).fillna(0)\n    conf_merge[\"count_prev\"] = conf_merge[\"count_prev\"].astype(int)\n    conf_merge[\"count_curr\"] = conf_merge[\"count_curr\"].astype(int)\n    conf_merge[\"d_count\"] = conf_merge[\"count_curr\"] - conf_merge[\"count_prev\"]\n    print(\"\\nTop pairs by absolute change in confusions:\")\n    print(conf_merge.sort_values(\"d_count\").head(12)[key+[\"count_prev\",\"count_curr\",\"d_count\"]])\n\n# CAM overlap deltas (per class)\nprev_cam_stat = prev_cam.groupby(\"label\")[\"overlap\"].mean().reset_index().rename(columns={\"overlap\":\"mean_prev\"})\ncurr_cam_stat = curr_cam.groupby(\"label\")[\"overlap\"].mean().reset_index().rename(columns={\"overlap\":\"mean_curr\"})\ncam_join = prev_cam_stat.merge(curr_cam_stat, on=\"label\", how=\"inner\")\ncam_join[\"d_overlap\"] = cam_join[\"mean_curr\"] - cam_join[\"mean_prev\"]\nprint(\"\\nClasses with largest CAM overlap gains:\")\nprint(cam_join.sort_values(\"d_overlap\", ascending=False).head(10))\nprint(\"\\nClasses with largest CAM overlap drops:\")\nprint(cam_join.sort_values(\"d_overlap\").head(10))\n\n# Save reports\nOUT = CURR/\"comparison_final\"\nOUT.mkdir(parents=True, exist_ok=True)\nrep_join.sort_values(\"d_f1\").to_csv(OUT/\"per_class_f1_delta.csv\", index=False)\nif prev_conf is not None:\n    conf_merge.sort_values(\"d_count\").to_csv(OUT/\"confusion_delta.csv\", index=False)\ncam_join.sort_values(\"d_overlap\").to_csv(OUT/\"cam_overlap_delta.csv\", index=False)\nprint(\"\\nSaved comparison files to:\", OUT)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}