{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":279449693,"sourceType":"kernelVersion"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **1. Purpose of This Notebook**\n\nAfter obtaining a robust Stage-6 EfficientNet-B3 baseline, this notebook focuses on:\n\n* Selective class curation\n* Fast adaptation to a reduced label space\n* Targeted robustness improvements for difficult classes\n* Final model selection\n\nThe goal here is not exploration, but controlled fine-tuning.\n\n## **2. Experiment Overview**\n\n**Fine-tune A**:\tCurate dataset by removing noisy / weak classes\n\n**Fine-tune B**:\tTargeted robustness fine-tuning with class weighting\n\nBoth experiments reuse the same backbone weights and same training utilities.\n\n## **3. Fine-Tune A — Class Curation & Label Space Reduction**\n3.1 Motivation\n\nSome classes in the original dataset:\n\n* have inconsistent annotations\n* are underrepresented\n* are visually ambiguous\n* or are not critical for the final objective\n\nRemoving them:\n\n* reduces label noise\n* simplifies the decision space\n* improves per-class performance on retained classes\n\n---\n\n## **Tomato — Special Case Within Class Curation**\n\nDuring the review phase, tomato classes stood out:\n\n* They were **well-represented** in the dataset\n* But also **highly confused among themselves** (blight vs. septoria vs. bacterial spot, etc.)\n* CAM analysis showed attention drifting toward **background textures instead of lesions**\n* UMAP revealed **overlapping feature clusters** for multiple tomato diseases\n\nBecause tomato is both **important** and **internally ambiguous**, it wasn’t a candidate for removal — instead, it required **focused refinement**.\n\nRather than retraining the whole network, we:\n\n1. **Extracted only tomato classes** into a smaller label space.\n2. **Loaded the global EfficientNet-B3 backbone** (frozen implicitly).\n3. **Trained only the classifier head** on tomato images.\n4. Oversampled the four most problematic diseases:\n\n   * tomato_bacterial_spot\n   * tomato_septoria_leaf_spot\n   * tomato_early_blight\n   * tomato_late_blight\n\nThis approach:\n\n* preserved the backbone’s global knowledge\n* allowed rapid specialization on subtle tomato lesion variations\n* reduced cross-class confusion without harming other crops\n\nWe validated the refinement using:\n\n* **Grad-CAM overlap** → attention shifted correctly toward diseased regions\n* **UMAP embeddings** → tomato clusters separated more clearly\n* **counterfactual edits** → lesion-enhancing transformations increased correct logits\n\nThe refined tomato head is then prepared for:\n\n* merging its classifier rows back into the unified model, or\n* deployment as a specialized sub-head for tomato-only routing.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"3.2 Build Curated Metadata","metadata":{}},{"cell_type":"code","source":"# Build curated train/val splits after removing unwanted classes\n\nimport pandas as pd, json\nfrom pathlib import Path\n\nMETADATA_CSV = \"outputs/metadata/metadata.csv\"\n\nDROP_CLASSES = {\n    \"diseased_rice\",\"guava_anthracnose\",\"guava_fruit_fly\",\n    \"healthy_corn\",\"healthy_guava\",\"healthy_rice\",\"healthy_sugarcane\",\"healthy_wheat\",\n    \"sugarcane_bacterial_blight\",\"sugarcane_mosaic\",\"sugarcane_red_rot\",\n    \"sugarcane_rust\",\"sugarcane_yellow_leaf_disease\",\n    \"wheat_aphid\",\"wheat_black_rust\",\"wheat_blast\",\"wheat_brown_rust\",\n    \"wheat_common_root_rot\",\"wheat_fusarium_head_blight\",\"wheat_leaf_blight\",\n    \"wheat_mildew\",\"wheat_mite\",\"wheat_septoria\",\"wheat_smut\",\n    \"wheat_stem_fly\",\"wheat_tan_spot\",\"wheat_yellow_rust\"\n}\n\nmeta = pd.read_csv(METADATA_CSV)\ncurated = meta[~meta[\"label\"].isin(DROP_CLASSES)].copy()\n\ntrain_df = curated[curated[\"split\"] == \"train\"]\nval_df   = curated[curated[\"split\"] == \"val\"]\n\nclasses = sorted(train_df[\"label\"].unique())\nlabel2idx = {c: i for i, c in enumerate(classes)}\n\nOUT_DIR = Path(\"outputs/metadata\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\ntrain_df.to_csv(OUT_DIR / \"curated_train.csv\", index=False)\nval_df.to_csv(OUT_DIR / \"curated_val.csv\", index=False)\nwith open(OUT_DIR / \"label2idx.json\", \"w\") as f:\n    json.dump(label2idx, f, indent=2)\n\nprint(f\"Curated classes: {len(classes)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **4. Fine-Tune A — Backbone Reuse + New Head**\n4.1 Key Idea\n\n* Reuse backbone weights from the best robust model\n* Re-initialize the classifier head for the new class count\n* Perform:\n    * short head-only warm-up\n    * followed by full fine-tuning with MixUp/CutMix","metadata":{}},{"cell_type":"markdown","source":"4.2 Curated Fine-Tune Training","metadata":{}},{"cell_type":"code","source":"# Load curated data\ntrain_df = pd.read_csv(\"outputs/metadata/curated_train.csv\")\nval_df   = pd.read_csv(\"outputs/metadata/curated_val.csv\")\n\nwith open(\"outputs/metadata/label2idx.json\") as f:\n    label2idx = json.load(f)\n\nnum_classes = len(label2idx)\n\ntrain_ds = PlantDataset(train_df, get_train_transform_hardened(), label2idx)\nval_ds   = PlantDataset(val_df,   get_val_transform(),           label2idx)\n\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\nval_loader   = DataLoader(val_ds, batch_size=64)\n\n# Initialize model\nmodel = create_efficientnet_b3(num_classes).to(DEVICE)\n\n# Load backbone weights only\nckpt = torch.load(\"outputs/checkpoints/effb3_320_std_domain_mix_v2.pt\", map_location=\"cpu\")\nstate = ckpt[\"model\"]\nmodel_state = model.state_dict()\nmodel_state.update({k: v for k, v in state.items() if not k.startswith(\"classifier\")})\nmodel.load_state_dict(model_state, strict=False)\n\ncriterion = LabelSmoothingCE(0.1)\nscaler = torch.amp.GradScaler(\"cuda\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Phase A — Head Warm-Up","metadata":{}},{"cell_type":"code","source":"set_requires_grad(model, False)\nset_requires_grad(model.classifier, True)\n\noptimizer = torch.optim.AdamW(model.classifier.parameters(), lr=3e-4)\n\nfor ep in range(2):\n    tr = train_one_epoch(model, train_loader, optimizer, scaler, criterion, DEVICE)\n    va_loss, va_acc, va_f1, va_top3, _ = validate(model, val_loader, criterion, DEVICE)\n    print(f\"[Warmup {ep+1}] val loss {va_loss:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Phase B — Full Fine-Tune","metadata":{}},{"cell_type":"code","source":"set_requires_grad(model, True)\n\noptimizer = torch.optim.AdamW(\n    get_param_groups(model, head_lr=1.5e-4, backbone_lr=5e-5, weight_decay=1e-2)\n)\n\nbest_val = float(\"inf\")\nbest_state = None\n\nfor ep in range(8):\n    tr = train_one_epoch_mix(\n        model, train_loader, optimizer, scaler, DEVICE,\n        mixup_alpha=0.3, cutmix_alpha=0.3\n    )\n    va_loss, va_acc, va_f1, va_top3, va_cm = validate(model, val_loader, criterion, DEVICE)\n\n    if va_loss < best_val:\n        best_val = va_loss\n        best_state = {\"model\": model.state_dict(), \"label2idx\": label2idx}\n\ntorch.save(best_state, \"outputs/checkpoints/effb3_320_curated.pt\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **5. Fine-Tune B — Targeted Robustness Training**\n5.1 Motivation\n\nSome classes consistently:\n\n* confuse with visually similar diseases\n* show higher variance across lighting/backgrounds\n\nInstead of re-balancing globally, you targeted these classes explicitly.\n\n5.2 Strategy\n\n* Use WeightedRandomSampler to up-sample focus classes\n* Apply hardened augmentations\n* Use milder MixUp/CutMix\n* Very short fine-tuning (avoid overfitting)","metadata":{}},{"cell_type":"markdown","source":"5.3 Targeted Fine-Tune","metadata":{}},{"cell_type":"code","source":"FOCUS_CLASSES = {\n    \"corn_common_rust\",\"bean_angular_leaf_spot\",\"bean_rust\",\n    \"diseased_cucumber\",\"healthy_cucumber\",\"healthy_bean\",\n    \"healthy_groundnut\",\"healthy_pumpkin\",\n    \"pumpkin_bacterial_leaf_spot\",\"pumpkin_downy_mildew\",\n    \"pumpkin_mosaic_disease\",\"pumpkin_powdery_mildew\"\n}\n\nweights = np.array([\n    1.5 if lbl in FOCUS_CLASSES else 1.0\n    for lbl in train_df[\"label\"]\n], dtype=np.float32)\n\nsampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n\ntrain_loader = DataLoader(train_ds, batch_size=32, sampler=sampler)\nval_loader   = DataLoader(val_ds, batch_size=64)\n\n# Resume from curated checkpoint\nmodel = create_efficientnet_b3(num_classes).to(DEVICE)\nckpt = torch.load(\"outputs/checkpoints/effb3_320_curated.pt\", map_location=\"cpu\")\nmodel.load_state_dict(ckpt[\"model\"], strict=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Short targeted fine-tune\noptimizer = torch.optim.AdamW(\n    get_param_groups(model, head_lr=1.5e-4, backbone_lr=5e-5, weight_decay=1e-2)\n)\n\nbest_val = float(\"inf\")\n\nfor ep in range(4):\n    tr = train_one_epoch_mix(\n        model, train_loader, optimizer, scaler, DEVICE,\n        mixup_alpha=0.2, cutmix_alpha=0.2\n    )\n    va_loss, va_acc, va_f1, va_top3, va_cm = validate(model, val_loader, criterion, DEVICE)\n\n    if va_loss < best_val:\n        best_val = va_loss\n        best_state = {\"model\": model.state_dict(), \"label2idx\": label2idx}\n\ntorch.save(best_state, \"outputs/checkpoints/effb3_320_curated_hardened.pt\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Section 4 — Cotton-Specific Investigation & Decision**\n\nThis section documents a focused investigation into cotton classes, which were found to negatively impact overall model performance despite multiple corrective attempts.\n\n### 4.1 Motivation: Why Cotton Was Investigated Separately\n\nDuring validation analysis of the curated and hardened models, cotton classes consistently showed:\n\n* unstable predictions\n* reduced generalization\n* negative spillover effects on non-cotton crops\n\nInitial inspection suggested that **many cotton images appeared black-and-white**, leading to the hypothesis that **color-based pretrained features were mismatched** for this species.\n\n### 4.2 Hypothesis 1 — Cotton Is Mostly Grayscale → Needs Special Handling\n\nAssumption\n\nIf cotton images are primarily grayscale, a cotton-only fine-tune with dedicated preprocessing may help the model adapt.\n\nThis motivated:\n\n* a **cotton-only audit**\n* removal of the worst cotton class (`cotton_aphids`)\n* grayscale / quality filtering\n* cotton-focused fine-tuning\n\n### 4.3 Cotton Audit & Cleaning (Evidence-Driven)\n\nWhat Was Checked\n\nFor **train and validation splits**, cotton images were audited for:\n\n* grayscale images\n* black borders / letterboxing\n* over-exposure\n* extreme quality issues\n\n#### Key Finding (Very Important)\n\n> Cotton images were **not consistently grayscale**.\n> Instead, they exhibited **mixed color spaces, heavy degradation, and poor acquisition quality**.\n\nThis invalidated the original hypothesis.\n\n#### Outcome\n\n* Significant portion of cotton data removed\n* Remaining data still **highly inconsistent**\n* Visual quality far below other crops\n\n### 4.4 Experiment 1 — Replace Cotton with Cleaned Subset\n\n#### Strategy\n\n* Keep all non-cotton data unchanged\n* Replace cotton rows with cleaned cotton images\n* Reuse **curated hardened backbone**\n* Apply **mild MixUp/CutMix**\n* Short fine-tuning with early stopping\n\n#### Intent\n\n> Test whether *partial cotton cleanup* can stop performance degradation.\n\n### 4.5 Experiment 2 — Cotton-Focused Fine-Tune (Final Attempt)\n\n#### Key Design Choices\n\n* Resume from **best curated hardened model**\n* Use **hardened augmentations**\n* Very **mild MixUp/CutMix**\n* Conservative learning rates\n* Early stopping\n* Weighted sampling to stabilize cotton\n\n## 4.6 Results & Failure Analysis\n\nDespite:\n\n* cotton-only cleaning\n* grayscale handling\n* class removal\n* hardened augmentations\n* weighted sampling\n* conservative fine-tuning\n\n### Observed Behavior\n\n* Validation loss oscillated\n* Confusion increased in **non-cotton classes**\n* Generalization degraded\n* Cotton features did not transfer reliably\n\n### Root Cause (Key Insight)\n\n> Cotton data quality was **systematically different** from other crops, not just grayscale.\n> Many images were:\n\n* poorly exposed\n* low resolution\n* inconsistently labeled\n* captured under non-representative conditions\n\nThis caused **negative transfer**.\n\n---\n\n## 4.7 Final Decision — Remove Cotton Entirely\n\n### Decision Rationale\n\n* Cotton classes were **hurting the model**\n* Fixing them required **dataset-level intervention**, not modeling tricks\n* Retaining cotton would reduce trustworthiness of the system\n\n### Final Action\n\n> **All cotton species were removed from the final training set.**\n\nThis decision was:\n\n* evidence-based\n* validated through multiple experiments\n* aligned with production ML best practices","metadata":{}},{"cell_type":"markdown","source":"4.1 Cotton Audit & Cleaning","metadata":{}},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm import tqdm\n\nTRAIN_ROOT = Path(\"/kaggle/input/plant-disease-detection-dataset-master-version/MasterDataset/train\")\nVAL_ROOT   = Path(\"/kaggle/input/plant-disease-detection-dataset-master-version/MasterDataset/val\")\n\nCOTTON_KEEP = [\n    \"cotton_bacterial_blight\",\n    \"cotton_powdery_mildew\",\n    \"cotton_target_spot\",\n    \"healthy_cotton\",\n]\nCOTTON_DROP = {\"cotton_aphids\"}\n\nEXTS = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\"}\n\ndef is_grayscale(img, tol=1):\n    b,g,r = cv2.split(img)\n    return (\n        np.max(np.abs(r.astype(int)-g.astype(int))) <= tol and\n        np.max(np.abs(r.astype(int)-b.astype(int))) <= tol\n    )\n\ndef has_black_bars(img, thr=5, frac=0.95):\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    return (\n        (gray[0,:] < thr).mean() > frac or\n        (gray[-1,:] < thr).mean() > frac or\n        (gray[:,0] < thr).mean() > frac or\n        (gray[:,-1] < thr).mean() > frac\n    )\n\ndef is_overexposed(img, v_thr=245, frac=0.15):\n    v = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)[...,2]\n    return (v > v_thr).mean() > frac\n\ndef scan_split(root):\n    rows = []\n    for cls in COTTON_KEEP:\n        d = root / cls\n        if not d.exists(): \n            continue\n        for p in d.rglob(\"*\"):\n            if p.suffix.lower() not in EXTS: \n                continue\n            img = cv2.imread(str(p))\n            if img is None: \n                continue\n            rows.append({\n                \"filepath\": str(p),\n                \"label\": cls,\n                \"grayscale\": is_grayscale(img),\n                \"black_bars\": has_black_bars(img),\n                \"overexposed\": is_overexposed(img)\n            })\n    return pd.DataFrame(rows)\n\ntrain_audit = scan_split(TRAIN_ROOT)\nval_audit   = scan_split(VAL_ROOT)\n\ntrain_audit.to_csv(\"cotton_audit_train.csv\", index=False)\nval_audit.to_csv(\"cotton_audit_val.csv\", index=False)\n\nprint(\"Train audit rows:\", len(train_audit))\nprint(\"Val audit rows:\", len(val_audit))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"4.2 Clean Cotton Subset","metadata":{}},{"cell_type":"code","source":"def clean_cotton(df):\n    keep = []\n    for _, r in tqdm(df.iterrows(), total=len(df)):\n        if r[\"grayscale\"] or r[\"overexposed\"]:\n            continue\n        keep.append(r)\n    return pd.DataFrame(keep)\n\nclean_train = clean_cotton(train_audit)\nclean_val   = clean_cotton(val_audit)\n\nclean_train[[\"filepath\",\"label\"]].to_csv(\"cotton_clean_train.csv\", index=False)\nclean_val[[\"filepath\",\"label\"]].to_csv(\"cotton_clean_val.csv\", index=False)\n\nprint(\"Clean train:\", len(clean_train))\nprint(\"Clean val:\", len(clean_val))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"4.3 Replace Cotton Rows in Curated Dataset","metadata":{}},{"cell_type":"code","source":"META_DIR = Path(\"outputs/metadata\")\n\ntrain_all = pd.read_csv(META_DIR / \"curated_train.csv\")\nval_all   = pd.read_csv(META_DIR / \"curated_val.csv\")\n\ncot_tr = pd.read_csv(\"cotton_clean_train.csv\")\ncot_va = pd.read_csv(\"cotton_clean_val.csv\")\n\ntrain_final = pd.concat([\n    train_all[~train_all[\"label\"].str.startswith(\"cotton\")],\n    cot_tr\n], ignore_index=True)\n\nval_final = pd.concat([\n    val_all[~val_all[\"label\"].str.startswith(\"cotton\")],\n    cot_va\n], ignore_index=True)\n\ntrain_final.to_csv(META_DIR / \"curated_train_cotton_fix.csv\", index=False)\nval_final.to_csv(META_DIR / \"curated_val_cotton_fix.csv\", index=False)\n\nprint(\"Final train:\", len(train_final))\nprint(\"Final val:\", len(val_final))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"4.4 Cotton-Focused Fine-Tune","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\n\nCKPT_DIR = Path(\"outputs/checkpoints\")\nBASE_CKPT = CKPT_DIR / \"effb3_320_curated_hardened.pt\"\n\ntrain_df = pd.read_csv(META_DIR / \"curated_train_cotton_fix.csv\")\nval_df   = pd.read_csv(META_DIR / \"curated_val_cotton_fix.csv\")\n\nlabel2idx = torch.load(BASE_CKPT, map_location=\"cpu\")[\"label2idx\"]\n\ntrain_ds = PlantDataset(train_df, get_train_transform_hardened(), label2idx)\nval_ds   = PlantDataset(val_df,   get_val_transform(),           label2idx)\n\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\nval_loader   = DataLoader(val_ds, batch_size=64)\n\nmodel = create_efficientnet_b3(len(label2idx)).to(DEVICE)\nmodel.load_state_dict(torch.load(BASE_CKPT)[\"model\"], strict=True)\n\ncriterion = LabelSmoothingCE(0.1)\nscaler = torch.amp.GradScaler(\"cuda\")\n\n# Warmup\nset_requires_grad(model, False)\nset_requires_grad(model.classifier, True)\nopt = torch.optim.AdamW(model.classifier.parameters(), lr=3e-4)\n\ntrain_one_epoch(model, train_loader, opt, scaler, criterion, DEVICE)\n\n# Fine-tune\nset_requires_grad(model, True)\nopt = torch.optim.AdamW(\n    get_param_groups(model, 1.5e-4, 5e-5, 1e-2)\n)\n\nbest = None\nbest_loss = float(\"inf\")\n\nfor ep in range(4):\n    tr = train_one_epoch_mix(\n        model, train_loader, opt, scaler, DEVICE,\n        mixup_alpha=0.12, cutmix_alpha=0.12\n    )\n    va_loss, va_acc, va_f1, va_top3, _ = validate(model, val_loader, criterion, DEVICE)\n\n    print(f\"[Cotton FT {ep+1}] val loss {va_loss:.4f}\")\n\n    if va_loss < best_loss:\n        best_loss = va_loss\n        best = {\"model\": model.state_dict(), \"label2idx\": label2idx}\n\ntorch.save(best, CKPT_DIR / \"effb3_320_curated_cotton_fix.pt\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Section 5: Tomato Head-Only Refinement — Full Experiment**","metadata":{}},{"cell_type":"markdown","source":"Setup & Data Filters (Tomato-only splits)","metadata":{}},{"cell_type":"code","source":"import os, json, torch, timm, numpy as np, pandas as pd\nfrom pathlib import Path\nimport cv2\nimport torch.nn.functional as F\nfrom torch import nn\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nIM_SIZE = 320\nIM_MEAN = [0.485, 0.456, 0.406]\nIM_STD  = [0.229, 0.224, 0.225]\nTEMP = 0.551\n\nTRAIN_CSV = \"/kaggle/working/outputs/metadata/curated_train_v6.csv\"\nVAL_CSV   = \"/kaggle/working/outputs/metadata/curated_val_v6.csv\"\nMAP_JSON  = \"/kaggle/working/outputs/metadata/label2idx_v6.json\"\nCKPT_PATH = \"/kaggle/input/k/adiithape/cnn-model-v3/outputs/checkpoints/effb3_320_curated_hardened_v7.pt\"\n\ntrain_df = pd.read_csv(TRAIN_CSV)\nval_df   = pd.read_csv(VAL_CSV)\nwith open(MAP_JSON, \"r\") as f: label2idx = json.load(f)\n\ndef is_tomato(lbl): return isinstance(lbl, str) and lbl.startswith(\"tomato_\")\n\ntrain_tom = train_df[train_df[\"label\"].apply(is_tomato)].reset_index(drop=True)\nval_tom   = val_df[val_df[\"label\"].apply(is_tomato)].reset_index(drop=True)\n\nprint(\"Tomato train / val:\", len(train_tom), len(val_tom))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Augmentations + Dataset + Loaders\nimport","metadata":{}},{"cell_type":"code","source":"import albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n\ndef get_tomato_train_tf():\n    return A.Compose([\n        A.RandomResizedCrop((IM_SIZE,IM_SIZE), (0.85,1.0), (0.9,1.1), cv2.INTER_AREA, p=1),\n        A.HorizontalFlip(p=0.5),\n        A.ShiftScaleRotate(0.02,0.05,20, border_mode=cv2.BORDER_REFLECT_101, p=0.6),\n        A.ColorJitter(0.12,0.12,0.12,0.05,p=0.6),\n        A.CLAHE(2.0,(8,8),p=0.3),\n        A.GaussianBlur(3,p=0.08),\n        A.ImageCompression(60,95,p=0.25),\n        A.Normalize(IM_MEAN,IM_STD),\n        ToTensorV2(),\n    ])\n\ndef get_val_tf():\n    return A.Compose([\n        A.LongestMaxSize(IM_SIZE),\n        A.PadIfNeeded(IM_SIZE,IM_SIZE),\n        A.Normalize(IM_MEAN,IM_STD),\n        ToTensorV2(),\n    ])\n\nclass TomatoDS(Dataset):\n    def __init__(self, df, l2i, tf):\n        self.df=df.reset_index(drop=True); self.l2i=l2i; self.tf=tf\n    def __len__(self): return len(self.df)\n    def __getitem__(self,i):\n        r=self.df.iloc[i]\n        img=cv2.imread(r[\"filepath\"]); img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n        return self.tf(image=img)[\"image\"], self.l2i[r[\"label\"]]\n\ntom_labels = sorted(pd.concat([train_tom[\"label\"], val_tom[\"label\"]]).unique())\nl2i_tom = {l:i for i,l in enumerate(tom_labels)}\n\ntrain_ds = TomatoDS(train_tom, l2i_tom, get_tomato_train_tf())\nval_ds   = TomatoDS(val_tom,   l2i_tom, get_val_tf())\n\nw = np.where(train_tom[\"label\"].isin({\n    \"tomato_bacterial_spot\",\"tomato_septoria_leaf_spot\",\n    \"tomato_early_blight\",\"tomato_late_blight\"\n}),2.5,1.0)\n\nsampler = WeightedRandomSampler(w, len(w), replacement=True)\n\ntrain_loader = DataLoader(train_ds,32,sampler=sampler)\nval_loader   = DataLoader(val_ds,64,shuffle=False)\n\nprint(\"Tomato labels:\", tom_labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Train Tomato-only head (freeze backbone)","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nfrom copy import deepcopy\n\nmodel = timm.create_model(\"efficientnet_b3\", pretrained=False, num_classes=len(l2i_tom)).to(DEVICE)\nbase = torch.load(CKPT_PATH,map_location=\"cpu\")\nstate = base[\"model\"] if isinstance(base,dict) and \"model\" in base else base\n\nsd=model.state_dict(); loaded=0\nfor k,v in state.items():\n    if not k.startswith(\"classifier\") and k in sd and sd[k].shape==v.shape:\n        sd[k]=v; loaded+=1\nmodel.load_state_dict(sd, strict=False)\nprint(\"Backbone tensors loaded:\", loaded)\n\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.06)\nopt = torch.optim.AdamW([\n    {\"params\":[p for n,p in model.named_parameters() if \"classifier\" not in n], \"lr\":5e-5},\n    {\"params\":[p for n,p in model.named_parameters() if \"classifier\" in n],      \"lr\":1.5e-4},\n])\n\nbest = {\"loss\":1e9,\"state\":deepcopy(model.state_dict())}\n\ndef run_epoch(loader):\n    model.train(); total=0\n    for x,y in loader:\n        x,y=x.to(DEVICE), y.to(DEVICE)\n        opt.zero_grad(); loss=criterion(model(x),y)\n        loss.backward(); opt.step()\n        total+=loss.item()*x.size(0)\n    return total/len(loader.dataset)\n\n@torch.no_grad()\ndef evaluate(loader):\n    model.eval(); tot=0; hit=0\n    for x,y in loader:\n        x,y=x.to(DEVICE),y.to(DEVICE)\n        z=model(x); tot+=criterion(z,y).item()*x.size(0)\n        hit+=(z.argmax(1)==y).sum().item()\n    return tot/len(loader.dataset), hit/len(loader.dataset)\n\nfor ep in range(2):\n    tr = run_epoch(train_loader)\n    vl, acc = evaluate(val_loader)\n    print(f\"ep {ep+1} | train {tr:.4f} | val {vl:.4f} | acc {acc:.4f}\")\n    if vl < best[\"loss\"]:\n        best = {\"loss\":vl,\"state\":deepcopy(model.state_dict())}\n\nmodel.load_state_dict(best[\"state\"])\nOUT = Path(\"/kaggle/working/tomato_refine\"); OUT.mkdir(exist_ok=True)\nTOM_CKPT = OUT/\"effb3_tomato_refined.pt\"\ntorch.save({\"model\":model.state_dict(),\"label2idx\":l2i_tom}, TOM_CKPT)\nprint(\"Saved:\", TOM_CKPT)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Quick results + CAM sanity check","metadata":{}},{"cell_type":"code","source":"def top1(csv): return (pd.read_csv(csv)[\"pred\"]==pd.read_csv(csv)[\"gt\"]).mean()\nfor T in [0.5,0.6,0.7]:\n    print(T, top1(OUT/f\"tomato_val_refined_T{T}.csv\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch import nn\ndef get_last_conv(m):\n    last=None\n    for _,mod in m.named_modules():\n        if isinstance(mod, nn.Conv2d): last=mod\n    return last\n\ndef lesion_mask(img):\n    L=cv2.createCLAHE(2.0,(8,8)).apply(cv2.cvtColor(img,cv2.COLOR_BGR2LAB)[:,:,0])\n    e=cv2.Canny(L,60,120); e=cv2.dilate(e,np.ones((3,3),np.uint8),1)\n    return cv2.resize((e>0).astype(np.uint8),(IM_SIZE,IM_SIZE))\n\ntarget=get_last_conv(model_t)\n\ndef gradcam(img,cls):\n    acts=[]; grads=[]\n    def f(m,i,o): acts.append(o.detach())\n    def b(m,gi,go): grads.append(go[0].detach())\n    h1=target.register_forward_hook(f); h2=target.register_full_backward_hook(b)\n    x=dual_logits(model_t,img,None).unsqueeze(0).to(DEVICE)\n    model_t.zero_grad(); out=model_t(x); out[0,cls].backward()\n    h1.remove(); h2.remove()\n    a=acts[-1][0]; g=grads[-1][0]; w=g.mean((1,2),True)\n    cam=(w*a).sum(0).cpu().numpy(); cam=np.maximum(cam,0)\n    cam=cv2.resize(cam,(IM_SIZE,IM_SIZE))\n    return (cam-cam.min())/(cam.max()+1e-6)\n\nrows=[]\nfocus = [\n \"tomato_bacterial_spot\",\"tomato_septoria_leaf_spot\",\n \"tomato_early_blight\",\"tomato_late_blight\"\n]\n\nfor lbl in focus:\n    sub=val_tom[val_tom[\"label\"]==lbl].sample(min(12,len(val_tom)),random_state=42)\n    for _,r in sub.iterrows():\n        img=cv2.imread(r[\"filepath\"])\n        m=lesion_mask(img); c=gradcam(img,l2i_tom[lbl])\n        rows.append({\"label\":lbl,\"filepath\":r[\"filepath\"],\n                     \"overlap\":float((c*(m>0)).sum()/(c.sum()+1e-6))})\n\npd.DataFrame(rows).to_csv(OUT/\"tomato_cam_overlap_refined.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"UMAP","metadata":{}},{"cell_type":"code","source":"# ===== Tomato UMAP — robust, self-contained =====\nimport numpy as np, pandas as pd, torch, cv2, importlib\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\n# 0) Safety — remove any weird sklearn / UMAP monkey-patches\nimport sklearn.utils.validation as suv\nimport umap as umap_module\nimportlib.reload(suv)\nimportlib.reload(umap_module)\nimport umap  # clean handle\n\n# 1) Safe penultimate feature extraction (no AMP, temporary hook)\n@torch.no_grad()\ndef penultimate_feature(img_bgr: np.ndarray) -> np.ndarray:\n    feats = []\n\n    def hook(module, inp, out):\n        feats.append(out.detach().cpu())\n\n    handle = model.global_pool.register_forward_hook(hook)\n\n    x = preprocess_bgr_center_crop(img_bgr).unsqueeze(0).to(DEVICE).float()\n    model.eval()(x)\n\n    handle.remove()\n    assert len(feats) == 1, \"Feature hook failed.\"\n\n    v = feats[0].squeeze(0).numpy().astype(np.float32)\n\n    # Defensive cleanup\n    if not np.all(np.isfinite(v)):\n        v = np.nan_to_num(v, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n\n    return v\n\n# 2) Balanced selection + clean matrix builder\ndef extract_features_clean(df: pd.DataFrame, per_class_cap=200, random_state=42):\n    parts = []\n    for lbl, grp in df.groupby(\"label\"):\n        parts.append(grp.sample(min(per_class_cap, len(grp)), random_state=random_state))\n    sub = pd.concat(parts, ignoreindex=True)\n\n    feats, labels, files = [], [], []\n\n    for _, r in tqdm(sub.iterrows(), total=len(sub), desc=\"Extract tomato feats\"):\n        fp = r[\"filepath\"]\n        img = load_rgb(fp)\n        v = penultimate_feature(img)\n        feats.append(v)\n        labels.append(r[\"label\"])\n        files.append(fp)\n\n    X = np.vstack(feats).astype(np.float32)\n    y = np.array(labels)\n    fpaths = np.array(files)\n\n    # Final cleanup\n    X[~np.isfinite(X)] = 0.0\n    return X, y, fpaths\n\n# 3) Extract tomato validation embeddings\nX, y, fpaths = extract_features_clean(val_tom, per_class_cap=200)\n\n# 4) UMAP embedding (cosine)\nreducer = umap.UMAP(\n    n_neighbors=15,\n    min_dist=0.1,\n    metric=\"cosine\",\n    random_state=42\n)\nU = reducer.fit_transform(X)\n\n# 5) Plot & save\nplot_df = pd.DataFrame({\n    \"x\": U[:,0],\n    \"y\": U[:,1],\n    \"label\": y,\n    \"filepath\": fpaths\n})\n\nplt.figure(figsize=(7,6))\nfor lbl in sorted(pd.unique(plot_df[\"label\"])):\n    pts = plot_df[plot_df[\"label\"] == lbl]\n    plt.scatter(pts[\"x\"], pts[\"y\"], s=10, label=lbl, alpha=0.75)\n\nplt.legend(markerscale=2, fontsize=8)\nplt.title(\"UMAP — Tomato diseases (val)\")\nplt.tight_layout()\n\npng_path = OUT_DIR / \"tomato_umap_val.png\"\nplt.savefig(png_path, dpi=200)\nplt.close()\nprint(\"Saved UMAP:\", png_path)\n\n# 6) Save artifacts\nnp.save(OUT_DIR/\"tomato_X.npy\", X)\npd.Series(y).to_csv(OUT_DIR/\"tomato_y.csv\", index=False, header=False)\npd.Series(fpaths).to_csv(OUT_DIR/\"tomato_files.csv\", index=False, header=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}